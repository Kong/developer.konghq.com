metadata:
  title: "Kong AI Gateway"
  content_type: landing_page
  description: This page is an introduction to Kong AI Gateway.
  products:
    - ai-gateway
    - gateway
  works_on:
    - on-prem
    - konnect
  tags:
    - ai

rows:
  - header:
      type: h1
      text: "Kong AI Gateway"
      sub_text: Connectivity and governance layer for modern AI-native applications built on top of {{site.base_gateway}}
  - columns:
      - blocks:
          - type: structured_text
            config:
              header:
                text: "Introducing Kong AI Gateway"
              blocks:
                - type: text
                  text: |
                    As AI adoption accelerates, applications are evolving beyond basic LLM calls into complex, multi-actor systems-including user apps, agents, orchestration layers, and context servers that interact with foundation models in real time.

                    To support this shift, developers are adopting protocols like Model Context Protocol (MCP) and Agent2Agent (A2A) to standardize how components exchange tools, data, and decisions.

                    But infrastructure often falls behind, with challenges around authentication, rate limiting, data security, observability, and constant provider changes.

                    Kong AI Gateway addresses these challenges with a high-performance control plane that secures, governs, and observes AI-native systems end to end. Whether serving LLM traffic, exposing structured context via MCP, or coordinating agents through A2A, Kong AI Gateway ensures scalable, secure, and reliable AI infrastructure.


      - blocks:
          - type: image
            config:
              url: /assets/images/gateway/ai-gateway-overview.svg
              alt_text: Overview of AI gateway
  - columns:
      - blocks:
        - type: structured_text
          config:
            header:
              text: "Quickstart"
            blocks:
              - type: text
                text: |
                  Launch a [demo instance](/gateway/quickstart-reference/#ai-gateway-quickstart) of {{site.base_gateway}} running AI Proxy:
                  ```sh
                  curl -Ls https://get.konghq.com/ai | bash
                  ```

  - columns:
      - blocks:
        - type: card
          config:
            title: Get started
            description: Run the {{site.base_gateway}} quickstart and enable the AI Proxy plugin.
            icon: /assets/icons/rocket.svg
            cta:
              url: /ai-gateway/get-started/
              align: end
      - blocks:
        - type: card
          config:
            title: Video tutorials
            description: Learn how to use AI plugins with video tutorials.
            icon: /assets/icons/graduation.svg
            cta:
              url: https://konghq.com/products/kong-ai-gateway#videos
              align: end
      - blocks:
        - type: card
          config:
            title: AI plugins
            description: Learn about all the AI plugins.
            icon: /assets/icons/plug.svg
            cta:
              url: /plugins/?category=ai
              align: end
      - blocks:
        - type: card
          config:
            title: AI providers
            description: Learn about the various providers supported by AI Gateway.
            icon: /assets/icons/ai.svg
            cta:
              url: /ai-gateway/ai-providers/
              align: end


  - header:
      type: h2
      text: "Tools to manage AI Gateway"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    Kong AI Gateway, working alongside {{site.base_gateway}}, supports multiple tools for managing configuration and resources. Use the following tools to automate, integrate, or streamline AI Gateway operations in a way that best fits your deployment model.
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: unordered_list
                  items:
                    - "[AI Manager](/ai-manager/): GUI for managing all your Kong AI Gateway resources in one place."
                    - "[decK](/deck/): Manage Kong AI Gateway and {{site.base_gateway}} configuration through declarative state files."
                    - "[Terraform](/terraform/): Manage infrastructure as code and automated deployments to streamline setup and configuration of {{site.konnect_short_name}} and {{site.base_gateway}}."
                    - "[KIC](/kubernetes-ingress-controller/): Manage ingress traffic and routing rules for your services."
                    - "[{{site.base_gateway}} Admin API](/api/gateway/admin-ee/): Manage on-prem {{site.base_gateway}} entities via an API."
                    - "[Control Plane Config API](/api/konnect/control-planes-config/): Manage {{site.base_gateway}} entities within {{site.konnect_short_name}} Control Planes via an API."
  - header:
      type: h2
      text: "AI Gateway capabilities"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    You can enable the AI Gateway features through a set of modern and specialized plugins, using the same model you use for any other {{site.base_gateway}} plugin.
                    When deployed alongside existing {{site.base_gateway}} plugins, {{site.base_gateway}} users can quickly assemble a sophisticated AI management platform without custom code or deploying new and unfamiliar tools.
  - columns:
    - blocks:
      - type: card
        config:
          title: Universal API
          description: Route client requests to various AI providers.
          icon: /assets/icons/plugins/universal-api.svg
          cta:
            url: ./#universal-api
            align: end
    - blocks:
      - type: card
        config:
          title: Rate limiting
          description: Manage traffic to your LLM API.
          icon: /assets/icons/plugins/ai-rate-limiting-advanced.png
          cta:
            url: /plugins/ai-rate-limiting-advanced/
            align: end
    - blocks:
      - type: card
        config:
          title: Semantic caching
          description: Semantically cache responses from LLMs.
          icon: /assets/icons/plugins/ai-semantic-cache.png
          cta:
            url: /plugins/ai-semantic-cache/
            align: end
  - columns:
      - blocks:
        - type: card
          config:
            title: Semantic routing
            description: Semantically distribute requests to different LLM models.
            icon: /assets/icons/plugins/ai-proxy-advanced.png
            cta:
              url: /plugins/ai-proxy-advanced/examples/semantic/
              align: end
      - blocks:
        - type: card
          config:
            title: MCP traffic gateway
            description: Gain control and visibility over AI agent infrastructure with AI Gateway-driven MCP capabilities
            icon: /assets/icons/mcp.svg
            cta:
              url: /mcp
              align: end
      - blocks:
        - type: card
          config:
            title: Automated RAG injection
            description: Automatically embed RAG logic into your workflows.
            icon: /assets/icons/plugins/ai-rag-injector.png
            cta:
              url: ./#automated-rag
              align: end
  - columns:
      - blocks:
        - type: card
          config:
            title: Data governance
            description: Use AI plugins to control AI data and usage.
            icon: /assets/icons/security.svg
            cta:
              url: ./#data-governance
              align: end
      - blocks:
        - type: card
          config:
            title: Guardrails
            description: Inspect requests and configure content safety and moderation.
            icon: /assets/icons/lock.svg
            cta:
              url: ./#guardrails-and-content-safety
              align: end
      - blocks:
        - type: card
          config:
            title: Prompt engineering
            description: Create prompt templates and manipulate client prompts.
            icon: /assets/icons/code.svg
            cta:
              url: ./#prompt-engineering
              align: end
  - columns:
      - blocks:
        - type: card
          config:
            title: Load balancing
            description: Learn about the load balancing algorithms available for AI Gateway.
            icon: /assets/icons/load-balance.svg
            cta:
              url: ./#load-balancing
              align: end
      - blocks:
        - type: card
          config:
            title: Audit log
            description: Learn about AI Gateway logging capabilities.
            icon: /assets/icons/audit.svg
            cta:
              url: /ai-gateway/ai-audit-log-reference/
              align: end
      - blocks:
        - type: card
          config:
            title: LLM metrics
            description: Expose and visualize LLM metrics.
            icon: /assets/icons/monitor.svg
            cta:
              url: /ai-gateway/monitor-ai-llm-metrics/
              align: end
  - columns:
      - blocks:
        - type: card
          config:
            title: '{{site.konnect_short_name}} Advanced Analytics'
            description: Visualize LLM metrics in {{site.konnect_short_name}}.
            icon: /assets/icons/analytics.svg
            cta:
              url: /advanced-analytics/
              align: end
      - blocks:
        - type: card
          config:
            title: Streaming
            description: Stream user requests with AI Gateway
            icon: /assets/icons/network.svg
            cta:
              url: /ai-gateway/streaming/
              align: end
      - blocks:
        - type: card
          config:
            title: Secrets management
            description: Use Konnect Config Store to store and reference your LLM provider API keys
            icon: /assets/icons/lock.svg
            cta:
              url: /how-to/configure-the-konnect-config-store/
              align: end
  - columns:
      - blocks:
        - type: card
          config:
            title: Prompt compression
            description: Keep your prompts lean, reduce latency, and optimize LLM usage for cost efficiency
            icon: /assets/icons/plugins/ai-prompt-compressor.png
            cta:
              url: /plugins/ai-prompt-compressor
              align: end
      - blocks:
        - type: card
          config:
            title: LLM cost control
            description: Reduce LLM usage costs by giving you control over how prompts are built and routed
            icon: /assets/icons/money.svg
            cta:
              url: ./#llm-cost-control
              align: end
      - blocks:
        - type: card
          config:
            title: Request transformations
            description: Use AI to transform requests and responses.
            icon: /assets/icons/plugins/ai-request-transformer.png
            cta:
              url: ./#request-transformations
              align: end


  - header:
    type: h2
  - columns:
    - blocks:
        - type: structured_text
          config:
            header:
              text: "Universal API"
            blocks:
              - type: text
                text: |
                  Kong’s AI Gateway Universal API, delivered through the [AI Proxy](/plugins/ai-proxy/) and [AI Proxy Advanced](/plugins/ai-proxy-advanced/) plugins, simplifies AI model integration by providing a single, standardized interface for interacting with models across multiple providers.

                    - [**Easy to use**](/plugins/ai-proxy/examples/openai-chat-route/): Configure once and access any AI model with minimal integration effort.

                    - [**Load balancing**](/plugins/ai-proxy-advanced/#load-balancing): Automatically distribute AI requests across multiple models or providers for optimal performance and cost efficiency.

                    - [**Retry and fallback**](/plugins/ai-proxy-advanced/#retry-and-fallback): Optimize AI requests based on model performance, cost, or other factors.

                    - [**Cross-plugin integration**](/how-to/visualize-ai-gateway-metrics-with-kibana/): Leverage AI in non-AI API workflows through other Kong Gateway plugins.

    - blocks:
        - type: image
          config:
            url: /assets/images/gateway/universal-api.svg
            alt_text: Overview of AI gateway
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-proxy
      - blocks:
        - type: plugin
          config:
            slug: ai-proxy-advanced

  - header:
    type: h2
  - columns:
      - blocks:
          - type: structured_text
            config:
              header:
                text: "AI Usage Governance"
              blocks:
                - type: text
                  text: |
                    As AI technologies see broader adoption, developers and organizations face new risks: the risk of sensitive data leaking to AI providers, which exposes businesses and their customers to potential breaches and security threats.

                    Managing how data flows to and from AI models has become critical not just for security, but also for compliance and reliability. Without the right controls in place, organizations risk losing visibility into how AI is used across their systems.

      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    <p/><p/>
                    Kong AI Gateway helps mitigate these challenges by offering a suite of plugins that extend beyond basic AI traffic management.

                    - [**Data governance**](./#data-governance): Control how sensitive information is handled and shared with AI models.
                    - [**Prompt engineering**](./#prompt-engineering): Customize and optimize prompts to deliver consistent, high-quality AI outputs.
                    - [**Guardrails and content safety**](./#guardrails-and-content-safety): Enforce policies to prevent inappropriate, unsafe, or non-compliant responses.
                    - [**Automated RAG injection**](./#automated-rag): Seamlessly inject relevant, vetted data into AI prompts without manual RAG implementations.
                    - [**Load balancing**](./#load-balancing): Distribute AI traffic efficiently across multiple model endpoints to ensure performance and reliability.
                    - [**LLM cost control**](./#llm-cost-control): Use the AI Compressor, RAG Injector, and Prompt Decorator to compress and structure prompts efficiently. Combine with AI Proxy Advanced to route requests across OpenAI models by semantic similarity—optimizing for cost and performance.
  - header:
      type: h3
      text: "Data governance"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                      AI Gateway enforces governance on outgoing AI prompts through allow/deny lists, blocking unauthorized requests with 4xx responses. It also provides built-in PII sanitization, automatically detecting and redacting sensitive data across 20+ categories and 12 languages. Running privately and self-hosted for full control and compliance, AI Gateway ensures consistent protection without burdening developers, which helps simplify AI adoption at scale.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-guard
      - blocks:
        - type: plugin
          config:
            slug: ai-semantic-prompt-guard
      - blocks:
        - type: plugin
          config:
            slug: ai-sanitizer

  - header:
      type: h3
      text: "Prompt engineering"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    AI systems are built around prompts, and manipulating those prompts is important for successful adoption of the technologies.
                    Prompt engineering is the methodology of manipulating the linguistic inputs that guide the AI system.
                    Kong AI Gateway supports a set of plugins that allow you to create a simplified and enhanced experience by setting default prompts or manipulating prompts from clients as they pass through the gateway.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-template
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-decorator

  - header:
      type: h3
      text: "Guardrails and content safety"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    As a platform owner, you may need to moderate all user request content against reputable services to comply with specific sensitive categories when proxying Large Language Model (LLM) traffic. Kong AI Gateway provides built-in capabilities to handle content moderation and ensure content safety, that help you enforce compliance and protect your users across AI-powered applications.
  - column_count: 3
    columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-azure-content-safety
      - blocks:
        - type: plugin
          config:
            slug: ai-aws-guardrails
      - blocks:
        - type: plugin
          config:
            slug: ai-gcp-model-armor
      - blocks:
        - type: plugin
          config:
            slug: ai-semantic-prompt-guard
      - blocks:
        - type: plugin
          config:
            slug: ai-semantic-response-guard
      - blocks:
        - type: card
          config:
            title: Amazon Bedrock guardrails
            description: Include your Amazon Bedrock guardrails configuration in AI Proxy requests
            icon: /assets/icons/bedrock.svg
            cta:
              url: /plugins/ai-proxy/#input-formats
              align: end

  - header:
      type: h3
      text: "Request transformations"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    AI Gateway allows you to use AI technology to augment other API traffic.
                    One example is routing API responses through an AI language translation prompt before returning it to the client.
                    Kong AI Gateway provides two plugins that can be used in conjunction with other upstream API services to weave AI capabilities into API request processing.
                    These plugins can be configured independently of the AI Proxy plugin.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-request-transformer
      - blocks:
        - type: plugin
          config:
            slug: ai-response-transformer


  - header:
      type: h3
      text: "Automated RAG"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    LLMs are only as reliable as the data they can access. When faced with incomplete information, they often produce confident yet incorrect responses known as “hallucinations.” These hallucinations occur when LLMs lack the necessary domain knowledge.To address this, developers use the **Retrieval-augmented Generation (RAG)** approach, which enriches models with relevant data pulled from vector databases.

                    While standard RAG workflows are resource-heavy, as they require teams to generate embeddings and manually curate them in vector databases, Kong’s **AI RAG Injector** plugin automates this entire process. Instead of embedding RAG logic into every application individually, platform teams can inject vetted data into prompts directly at the gateway layer without any manual interventions.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-rag-injector

  - header:
      type: h3
      text: "Load balancing"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    AI Gateway’s load balancer routes requests across AI models to optimize for speed, cost, and reliability. It supports algorithms like consistent hashing, lowest-latency, usage-based, round-robin, and semantic matching, with built-in retries and fallback for resilience {% new_in 3.10%}. The balancer dynamically selects models based on real-time performance and prompt relevance, and works across mixed environments including OpenAI, Mistral, and Llama models.

  - columns:
      - blocks:
        - type: card
          config:
            title: Load balancing
            description: Learn about the load balancing algorithms available for AI Gateway.
            icon: /assets/icons/load-balance.svg
            cta:
              url: /ai-gateway/load-balancing/
              align: end
      - blocks:
        - type: card
          config:
            title: Retry and fallback
            description: Learn about how AI Gateway load balancers handle retry and fallback.
            icon: /assets/icons/redo.svg
            cta:
              url: /ai-gateway/load-balancing/#retry-and-fallback
              align: end
  - header:
      type: h3
      text: "LLM cost control"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    The AI Gateway helps reduce LLM usage costs by giving you control over how prompts are built and routed. You can compress and structure prompts efficiently using the AI Compressor, RAG Injector, and AI Prompt Decorator plugins. For further savings, you can use AI Proxy Advanced to route requests across OpenAI models based on semantic similarity.

  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-compressor
      - blocks:
        - type: card
          config:
            title: Save LLM usage costs with semantic load balancing
            description: Use semantic load balancing to optimize LLM usage and reduce costs by intelligently routing chat requests across multiple OpenAI models based on semantic similarity.
            icon: /assets/icons/money.svg
            cta:
              url: /how-to/use-semantic-load-balancing
              align: end

  - header:
      type: h2
      text: How-to Guides

    columns:
      - blocks:
          - type: how_to_list
            config:
              tags:
                - ai
              quantity: 5
              allow_empty: true

  - header:
      text: "Frequently Asked Questions"
      type: h2
    columns:
      - blocks:
        - type: faqs
          config:
            - q: Is AI Gateway available for all deployment modes?
              a: |
                Yes, AI plugins are supported in all [deployment modes](/gateway/deployment-topologies/), including {{site.konnect_short_name}}, self-hosted traditional, hybrid, and DB-less, and on Kubernetes via the [{{site.kic_product_name}}](/kubernetes-ingress-controller/).

            - q: Why should I use AI Gateway instead of adding the LLM's API behind {{site.base_gateway}}?
              a: |
                If you just add an LLM's API behind {{site.base_gateway}}, you can only interact at the API level with internal traffic.
                With AI plugins, {{site.base_gateway}} can understand the prompts that are being sent through the gateway.
                The plugins can inspect the body and provide more specific AI capabilities to your traffic.

metadata:
  title: "{{site.ai_gateway_name}}"
  content_type: landing_page
  description: This page is an introduction to {{site.ai_gateway}}.
  products:
    - ai-gateway
    - gateway
  works_on:
    - on-prem
    - konnect
  tags:
    - ai

rows:
  - header:
      type: h1
      text: "{{site.ai_gateway}}"
      sub_text: Connectivity and governance layer for modern AI-native applications built on top of {{site.base_gateway}}
  - columns:
      - blocks:
          - type: structured_text
            config:
              header:
                text: "Introducing {{site.ai_gateway}}"
              blocks:
                - type: text
                  text: |
                    As AI adoption accelerates, applications are evolving beyond basic LLM calls into complex, multi-actor systems-including user apps, agents, orchestration layers, and context servers that interact with foundation models in real time.

                    To support this shift, developers are adopting protocols like Model Context Protocol (MCP) and Agent2Agent (A2A) to standardize how components exchange tools, data, and decisions.

                    But infrastructure often falls behind, with challenges around authentication, rate limiting, data security, observability, and constant provider changes.

                    {{site.ai_gateway}} addresses these challenges with a high-performance control plane that secures, governs, and observes AI-native systems end to end. Whether serving LLM traffic, exposing structured context via MCP, or coordinating agents through A2A, {{site.ai_gateway}} ensures scalable, secure, and reliable AI infrastructure.


      - blocks:
          - type: image
            config:
              url: /assets/images/gateway/ai-gateway-overview.svg
              alt_text: Overview of AI gateway

  - columns:
      - blocks:
        - type: structured_text
          config:
            header:
              text: "Quickstart"
            blocks:
              - type: text
                text: |
                  [Sign up for {{site.konnect_short_name}}](https://konghq.com/products/kong-konnect/register?utm_medium=referral&utm_source=docs&utm_content=ai-gateway) to get started with {{site.ai_gateway}}.

                  Or, launch a [demo instance](/gateway/quickstart-reference/#ai-gateway-quickstart) of {{site.ai_gateway}} running on-prem:
                  ```sh
                  curl -Ls https://get.konghq.com/ai | bash
                  ```

  - columns:
      - blocks:
        - type: card
          config:
            title: Get started
            description: Run the {{site.base_gateway}} quickstart and enable the AI Proxy plugin.
            icon: /assets/icons/rocket.svg
            cta:
              url: /ai-gateway/get-started/
              align: end
      - blocks:
        - type: card
          config:
            title: Video tutorials
            description: Learn how to use AI plugins with video tutorials.
            icon: /assets/icons/graduation.svg
            cta:
              url: https://konghq.com/products/kong-ai-gateway#videos
              align: end
      - blocks:
        - type: card
          config:
            title: AI plugins
            description: Learn about all the AI plugins.
            icon: /assets/icons/plug.svg
            cta:
              url: /plugins/?category=ai
              align: end

  - header:
    type: h2
  - columns:
    - blocks:
        - type: structured_text
          config:
            header:
              text: "{{site.ai_gateway}} providers"
            blocks:
              - type: text
                text: |
                  Kong AI Gatewy routes AI requests to various providers through a [provider-agnostic API](./#universal-api). This normalized API layer provides multiple benefits: client applications stay decoupled from provider-specific APIs, credentials are managed centrally, and request routing can be dynamic to optimize for cost, latency, or availability.

  - column_count: 4
    columns:
      - blocks:
        - type: icon_card
          config:
            title: OpenAI
            icon: /assets/icons/openai.svg
            cta:
              url: /ai-gateway/ai-providers/openai/
      - blocks:
        - type:  icon_card
          config:
            title: Anthropic
            icon: /assets/icons/anthropic.svg
            cta:
              url: /ai-gateway/ai-providers/anthropic/
      - blocks:
        - type:  icon_card
          config:
            title: Azure AI
            icon: /assets/icons/azure.svg
            cta:
              url: /ai-gateway/ai-providers/azure/
      - blocks:
        - type: icon_card
          config:
            title: More...
            icon: /assets/icons/dots.svg
            cta:
              url: /ai-gateway/ai-providers/
  - columns:
      - blocks:
          - type: structured_text
            config:
              header:
                text: "{{site.ai_gateway}} in {{site.konnect_short_name}}"
              blocks:
                - type: text
                  text: |
                    {{site.konnect_short_name}} provides a [unified control plane](https://cloud.konghq.com/ai-manager) to create, manage, and monitor LLMs
                    using the {{site.konnect_short_name}} platform.

                    Key features include:
                    * **Routing and [load balancing](/ai-gateway/load-balancing/)**: Assign Gateway Services and define how traffic is distributed across models.
                    * **Streaming and authentication**: Enable streaming responses and manage authentication through the {{site.ai_gateway}}.
                    * **Access control**: Create and apply access tiers to control how clients interact with LLMs.
                    * **Usage analytics**: Monitor request and token volumes, track error rates, and measure average latency with historical comparisons.
                    * **Visual traffic maps**: Explore interactive maps that show how requests flow between clients and models in real time.

      - blocks:
          - type: image
            config:
              url: /assets/images/konnect/ai-manager.png
              alt_text: "{{site.ai_gateway}} Dashboard in Konnect"

  - header:
    columns:
      - header:
          type: h2
          text: Deployment checklist
        blocks:
          - type: structured_text
            config:
              blocks:
                - type: unordered_list
                  items:
                    - "[{{site.ai_gateway}} resource sizing guidelines](/ai-gateway/resource-sizing-guidelines-ai/): Review recommended resource allocation guidelines for {{site.ai_gateway}}."
                    - "[Deployment topologies](/gateway/deployment-topologies/): Learn about the different ways to deploy {{ site.base_gateway }}."
                    - "[Hosting options](/gateway/topology-hosting-options/): Decide where you want to host your Data Plane nodes, and whether you want Kong to host them or host them yourself."
      - header:
          type: h2
          text: "Tools to manage {{site.ai_gateway}}"
        blocks:
          - type: structured_text
            config:
              blocks:
                - type: unordered_list
                  items:
                    - "[{{site.ai_gateway}} editor](https://cloud.konghq.com/ai-manager): GUI for managing all your {{site.ai_gateway}} resources in one place."
                    - "[decK](/deck/): Manage {{site.ai_gateway}} and {{site.base_gateway}} configuration through declarative state files."
                    - "[Terraform](/terraform/): Manage infrastructure as code and automated deployments to streamline setup and configuration of {{site.konnect_short_name}} and {{site.base_gateway}}."
                    - "[KIC](/kubernetes-ingress-controller/): Manage ingress traffic and routing rules for your services."
                    - "[{{site.base_gateway}} Admin API](/api/gateway/admin-ee/): Manage on-prem {{site.base_gateway}} entities via an API."
                    - "[Control Plane Config API](/api/konnect/control-planes-config/): Manage {{site.base_gateway}} entities within {{site.konnect_short_name}} Control Planes via an API."
  - header:
      type: h2
      text: "{{site.ai_gateway}} capabilities"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    You can enable the {{site.ai_gateway}} features through a set of modern and specialized plugins, using the same model you use for any other {{site.base_gateway}} plugin.
                    When deployed alongside existing {{site.base_gateway}} plugins, {{site.base_gateway}} users can quickly assemble a sophisticated AI management platform without custom code or deploying new and unfamiliar tools.
  - column_count: 3
    columns:
      - blocks:
        - type: card
          config:
            title: Universal API
            description: Route client requests to various AI providers
            icon: /assets/icons/plugins/universal-api.svg
            cta:
              url: ./#universal-api
              align: end
      - blocks:
        - type: card
          config:
            title: Rate limiting
            description: Manage traffic to your LLM API
            icon: /assets/icons/plugins/ai-rate-limiting-advanced.png
            cta:
              url: /plugins/ai-rate-limiting-advanced/
              align: end
      - blocks:
        - type: card
          config:
            title: Semantic caching
            description: Semantically cache responses from LLMs
            icon: /assets/icons/plugins/ai-semantic-cache.png
            cta:
              url: /plugins/ai-semantic-cache/
              align: end
      - blocks:
        - type: card
          config:
            title: Semantic routing
            description: Semantically distribute requests to different LLM models
            icon: /assets/icons/plugins/ai-proxy-advanced.png
            cta:
              url: /plugins/ai-proxy-advanced/examples/semantic/
              align: end
      - blocks:
        - type: card
          config:
            title: MCP traffic gateway
            description: Gain control and visibility over AI agent infrastructure with {{site.ai_gateway}}-driven MCP capabilities
            icon: /assets/icons/mcp.svg
            cta:
              url: /mcp
              align: end
      - blocks:
        - type: card
          config:
            title: Automated RAG injection
            description: Automatically embed RAG logic into your workflows
            icon: /assets/icons/plugins/ai-rag-injector.png
            cta:
              url: ./#automated-rag
              align: end
      - blocks:
        - type: card
          config:
            title: Data governance
            description: Use AI plugins to control AI data and usage
            icon: /assets/icons/security.svg
            cta:
              url: ./#data-governance
              align: end
      - blocks:
        - type: card
          config:
            title: Guardrails
            description: Inspect requests and configure content safety and moderation
            icon: /assets/icons/lock.svg
            cta:
              url: ./#guardrails-and-content-safety
              align: end
      - blocks:
        - type: card
          config:
            title: Prompt engineering
            description: Create prompt templates and manipulate client prompts
            icon: /assets/icons/code.svg
            cta:
              url: ./#prompt-engineering
              align: end
      - blocks:
        - type: card
          config:
            title: Load balancing
            description: Learn about the load balancing algorithms available for {{site.ai_gateway}}
            icon: /assets/icons/load-balance.svg
            cta:
              url: ./#load-balancing
              align: end
      - blocks:
        - type: card
          config:
            title: Audit log
            description: Learn about {{site.ai_gateway}} logging capabilities
            icon: /assets/icons/audit.svg
            cta:
              url: /ai-gateway/ai-audit-log-reference/
              align: end
      - blocks:
        - type: card
          config:
            title: LLM metrics
            description: Expose and visualize LLM metrics
            icon: /assets/icons/monitor.svg
            cta:
              url: ./#observability-and-metrics
              align: end
      - blocks:
        - type: card
          config:
            title: '{{site.konnect_short_name}} {{site.observability}}'
            description: Visualize LLM metrics in {{site.konnect_short_name}}.
            icon: /assets/icons/analytics.svg
            cta:
              url: /observability/llm-reporting/
              align: end
      - blocks:
        - type: card
          config:
            title: 'Metering & Billing'
            description: Meter LLM usage with {{site.konnect_short_name}}.
            icon: /assets/icons/monitor.svg
            cta:
              url: /how-to/meter-llm-traffic/
              align: end
      - blocks:
        - type: card
          config:
            title: Streaming
            description: Stream user requests with {{site.ai_gateway}}
            icon: /assets/icons/network.svg
            cta:
              url: /ai-gateway/streaming/
              align: end
      - blocks:
        - type: card
          config:
            title: Secrets management
            description: Use Konnect Config Store to store and reference your LLM provider API keys
            icon: /assets/icons/lock.svg
            cta:
              url: /how-to/configure-the-konnect-config-store/
              align: end
      - blocks:
        - type: card
          config:
            title: LLM cost control
            description: Reduce LLM usage costs by giving you control over how prompts are built and routed
            icon: /assets/icons/money.svg
            cta:
              url: ./#llm-cost-control
              align: end
      - blocks:
        - type: card
          config:
            title: Request transformations
            description: Use AI to transform requests and responses
            icon: /assets/icons/plugins/ai-request-transformer.png
            cta:
              url: ./#request-transformations
              align: end
      - blocks:
        - type: card
          config:
            title: Canary release
            description: Slowly roll out software changes to a subset of users.
            icon: /assets/icons/plugins/canary.png
            cta:
              url: /plugins/canary/
              align: end
      - blocks:
        - type: card
          config:
            title: Proxy AI CLI tools through {{site.ai_gateway}}
            description: Configure {{site.ai_gateway}} to proxy requests from AI command-line tools to LLM providers
            icon: /assets/icons/terminal.svg
            cta:
              url: /ai-gateway/ai-clis/
              align: end



  - header:
    type: h2
  - columns:
    - blocks:
        - type: structured_text
          config:
            header:
              text: "Universal API"
            blocks:
              - type: text
                text: |
                  Kong’s {{site.ai_gateway}} Universal API, delivered through the [AI Proxy](/plugins/ai-proxy/) and [AI Proxy Advanced](/plugins/ai-proxy-advanced/) plugins, simplifies AI model integration by providing a single, standardized interface for interacting with models across multiple providers.

                    - [**Easy to use**](/plugins/ai-proxy/examples/openai-chat-route/): Configure once and access any AI model with minimal integration effort.

                    - [**Load balancing**](/plugins/ai-proxy-advanced/#load-balancing): Automatically distribute AI requests across multiple models or providers for optimal performance and cost efficiency.

                    - [**Retry and fallback**](/plugins/ai-proxy-advanced/#retry-and-fallback): Optimize AI requests based on model performance, cost, or other factors.

                    - [**Cross-plugin integration**](/how-to/visualize-ai-gateway-metrics-with-kibana/): Leverage AI in non-AI API workflows through other Kong Gateway plugins.

    - blocks:
        - type: image
          config:
            url: /assets/images/gateway/universal-api.svg
            alt_text: Overview of AI gateway
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-proxy
      - blocks:
        - type: plugin
          config:
            slug: ai-proxy-advanced

  - header:
    type: h2
  - columns:
      - blocks:
          - type: structured_text
            config:
              header:
                text: "AI Usage Governance"
              blocks:
                - type: text
                  text: |
                    As AI technologies see broader adoption, developers and organizations face new risks: the risk of sensitive data leaking to AI providers, which exposes businesses and their customers to potential breaches and security threats.

                    Managing how data flows to and from AI models has become critical not just for security, but also for compliance and reliability. Without the right controls in place, organizations risk losing visibility into how AI is used across their systems.

      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    <p/><p/>
                    {{site.ai_gateway}} helps mitigate these challenges by offering a suite of plugins that extend beyond basic AI traffic management.

                    - [**Data governance**](./#data-governance): Control how sensitive information is handled and shared with AI models.
                    - [**Prompt engineering**](./#prompt-engineering): Customize and optimize prompts to deliver consistent, high-quality AI outputs.
                    - [**Guardrails and content safety**](./#guardrails-and-content-safety): Enforce policies to prevent inappropriate, unsafe, or non-compliant responses.
                    - [**Automated RAG injection**](./#automated-rag): Seamlessly inject relevant, vetted data into AI prompts without manual RAG implementations.
                    - [**Load balancing**](./#load-balancing): Distribute AI traffic efficiently across multiple model endpoints to ensure performance and reliability.
                    - [**LLM cost control**](./#llm-cost-control): Use the AI Compressor, RAG Injector, and Prompt Decorator to compress and structure prompts efficiently. Combine with AI Proxy Advanced to route requests across OpenAI models by semantic similarity—optimizing for cost and performance.
  - header:
      type: h3
      text: "Data governance"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                      {{site.ai_gateway}} enforces governance on outgoing AI prompts through allow/deny lists, blocking unauthorized requests with 4xx responses. It also provides built-in PII sanitization, automatically detecting and redacting sensitive data across 20 categories and 9 languages. Running privately and self-hosted for full control and compliance, {{site.ai_gateway}} ensures consistent protection without burdening developers, which helps simplify AI adoption at scale.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-guard
      - blocks:
        - type: plugin
          config:
            slug: ai-semantic-prompt-guard
      - blocks:
        - type: plugin
          config:
            slug: ai-sanitizer

  - header:
      type: h3
      text: "Prompt engineering"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    AI systems are built around prompts, and manipulating those prompts is important for successful adoption of the technologies.
                    Prompt engineering is the methodology of manipulating the linguistic inputs that guide the AI system.
                    {{site.ai_gateway}} supports a set of plugins that allow you to create a simplified and enhanced experience by setting default prompts or manipulating prompts from clients as they pass through the gateway.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-template
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-decorator

  - header:
      type: h3
      text: "Guardrails and content safety"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    As a platform owner, you may need to moderate all user request content against reputable services to comply with specific sensitive categories when proxying Large Language Model (LLM) traffic. {{site.ai_gateway}} provides built-in capabilities to handle content moderation and ensure content safety, that help you enforce compliance and protect your users across AI-powered applications.
  - column_count: 3
    columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-azure-content-safety
      - blocks:
        - type: plugin
          config:
            slug: ai-aws-guardrails
      - blocks:
        - type: plugin
          config:
            slug: ai-gcp-model-armor
      - blocks:
        - type: plugin
          config:
            slug: ai-semantic-prompt-guard
      - blocks:
        - type: plugin
          config:
            slug: ai-semantic-response-guard
      - blocks:
        - type: plugin
          config:
            slug: ai-lakera-guard
            icon: ai-lakera.png
      - blocks:
        - type: card
          config:
            title: Amazon Bedrock guardrails
            description: Include your Amazon Bedrock guardrails configuration in AI Proxy requests
            icon: /assets/icons/bedrock.svg
            cta:
              url: /plugins/ai-proxy/#input-formats
              align: end

  - header:
      type: h3
      text: "Request transformations"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    {{site.ai_gateway}} allows you to use AI technology to augment other API traffic.
                    One example is routing API responses through an AI language translation prompt before returning it to the client.
                    {{site.ai_gateway}} provides two plugins that can be used in conjunction with other upstream API services to weave AI capabilities into API request processing.
                    These plugins can be configured independently of the AI Proxy plugin.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-request-transformer
      - blocks:
        - type: plugin
          config:
            slug: ai-response-transformer


  - header:
      type: h3
      text: "Automated RAG"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    LLMs are only as reliable as the data they can access. When faced with incomplete information, they often produce confident yet incorrect responses known as “hallucinations.” These hallucinations occur when LLMs lack the necessary domain knowledge.To address this, developers use the **Retrieval-augmented Generation (RAG)** approach, which enriches models with relevant data pulled from vector databases.

                    While standard RAG workflows are resource-heavy, as they require teams to generate embeddings and manually curate them in vector databases, Kong’s **AI RAG Injector** plugin automates this entire process. Instead of embedding RAG logic into every application individually, platform teams can inject vetted data into prompts directly at the gateway layer without any manual interventions.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-rag-injector

  - header:
      type: h3
      text: "Load balancing"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    {{site.ai_gateway}}’s load balancer routes requests across AI models to optimize for speed, cost, and reliability. It supports algorithms like consistent hashing, lowest-latency, usage-based, round-robin, and semantic matching, with built-in retries and fallback for resilience {% new_in 3.10%}. The balancer dynamically selects models based on real-time performance and prompt relevance, and works across mixed environments including OpenAI, Mistral, and Llama models.

  - columns:
      - blocks:
        - type: card
          config:
            title: Load balancing
            description: Learn about the load balancing algorithms available for {{site.ai_gateway}}.
            icon: /assets/icons/load-balance.svg
            cta:
              url: /ai-gateway/load-balancing/
              align: end
      - blocks:
        - type: card
          config:
            title: Retry and fallback
            description: Learn about how {{site.ai_gateway}} load balancers handle retry and fallback.
            icon: /assets/icons/redo.svg
            cta:
              url: /ai-gateway/load-balancing/#retry-and-fallback
              align: end
  - header:
      type: h3
      text: "LLM cost control"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    The {{site.ai_gateway}} helps reduce LLM usage costs by giving you control over how prompts are built and routed. You can compress and structure prompts efficiently using the AI Compressor, RAG Injector, and AI Prompt Decorator plugins. For further savings, you can use AI Proxy Advanced to route requests across OpenAI models based on semantic similarity.

  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-compressor
      - blocks:
        - type: card
          config:
            title: Meter, bill, and monetize the entire AI connectivity data path
            description: Track LLM token usage across models and prompt types for accurate billing and cost control. Create pricing plans based on input, output, and system token consumption, then automate invoicing with Stripe or ERP integrations.
            icon: /assets/icons/analytics.svg
            cta:
              url: /metering-and-billing/
              align: end
      - blocks:
        - type: card
          config:
            title: Save LLM usage costs with semantic load balancing
            description: Use semantic load balancing to optimize LLM usage and reduce costs by intelligently routing chat requests across multiple OpenAI models based on semantic similarity.
            icon: /assets/icons/money.svg
            cta:
              url: /how-to/use-semantic-load-balancing
              align: end
  - header:
      type: h3
      text: "Observability and metrics"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    {{site.ai_gateway}} provides multiple approaches to monitor LLM traffic and operations. Track token usage, latency, and costs through audit logs and metrics exporters. Instrument request flows with OpenTelemetry to trace prompts and responses across your infrastructure. Use {{site.konnect_short_name}} Advanced Analytics for pre-built dashboards, or integrate with your existing observability stack.


  - columns:
      - blocks:
        - type: card
          config:
            title: Audit log
            description: Learn about {{site.ai_gateway}} logging capabilities.
            icon: /assets/icons/audit.svg
            cta:
              url: /ai-gateway/ai-audit-log-reference/
              align: end
      - blocks:
        - type: card
          config:
            title: '{{site.konnect_short_name}} {{site.observability}}'
            description: Visualize LLM metrics in {{site.konnect_short_name}}.
            icon: /assets/icons/analytics.svg
            cta:
              url: /observability/
              align: end
      - blocks:
        - type: card
          config:
            title: LLM metrics
            description: Expose and visualize LLM metrics.
            icon: /assets/icons/monitor.svg
            cta:
              url: /ai-gateway/monitor-ai-llm-metrics/
              align: end
      - blocks:
        - type: card
          config:
            title: Gen AI OpenTelemetry
            description: Expose and visualize LLM metrics.
            icon: /assets/icons/opentelemetry.svg
            cta:
              url: /ai-gateway/llm-open-telemetry/
              align: end

  - header:
      type: h2
      text: How-to Guides

    columns:
      - blocks:
          - type: how_to_list
            config:
              tags:
                - ai
              quantity: 5
              allow_empty: true

  - header:
      text: "Frequently Asked Questions"
      type: h2
    columns:
      - blocks:
        - type: faqs
          config:
            - q: Is {{site.ai_gateway}} available for all deployment modes?
              a: |
                Yes, AI plugins are supported in all [deployment modes](/gateway/deployment-topologies/), including {{site.konnect_short_name}}, self-hosted traditional, hybrid, and DB-less, and on Kubernetes via the [{{site.kic_product_name}}](/kubernetes-ingress-controller/).

            - q: Why should I use {{site.ai_gateway}} instead of adding the LLM's API behind {{site.base_gateway}}?
              a: |
                If you just add an LLM's API behind {{site.base_gateway}}, you can only interact at the API level with internal traffic.
                With AI plugins, {{site.base_gateway}} can understand the prompts that are being sent through the gateway.
                The plugins can inspect the body and provide more specific AI capabilities to your traffic.

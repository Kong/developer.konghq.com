metadata:
  title: "Kong AI Gateway"
  content_type: landing_page
  description: This page is an introduction to {{site.base_gateway}}.
  products:
    - ai-gateway
    - gateway
  works_on:
    - on-prem
    - konnect
  tags:
    - ai

rows:
  - header:
      type: h1
      text: "Kong AI Gateway"
      sub_text: Connectivity and governance layer for modern AI-native applications built on top of {{site.base_gateway}}
  - columns:
      - blocks:
          - type: structured_text
            config:
              header:
                text: "Introducing Kong AI Gateway"
              blocks:
                - type: text
                  text: |
                    As AI adoption accelerates, applications are evolving beyond basic LLM calls into complex, multi-actor systems—including user apps, agents, orchestration layers, and context servers—all interacting with foundation models in real time.

                    To support this shift, developers are adopting protocols like Model Context Protocol (MCP) and Agent2Agent (A2A) to standardize how components exchange tools, data, and decisions.

                    But infrastructure often falls behind, with challenges around authentication, rate limiting, data security, observability, and constant provider changes.

                    Kong AI Gateway addresses these challenges with a high-performance control plane that secures, governs, and observes AI-native systems end to end. Whether serving LLM traffic, exposing structured context via MCP, or coordinating agents through A2A, Kong AI Gateway ensures scalable, secure, and reliable AI infrastructure.


      - blocks:
          - type: image
            config:
              url: /assets/images/gateway/ai-gateway-overview.svg
              alt_text: Overview of AI gateway
  - columns:
      - blocks:
        - type: structured_text
          config:
            header:
              text: "Quickstart"
            blocks:
              - type: text
                text: |
                  Launch a [demo instance](/gateway/quickstart-reference/#ai-gateway-quickstart) of {{site.base_gateway}} running AI Proxy:
                  ```sh
                  curl -Ls https://get.konghq.com/ai | bash
                  ```

  - columns:
      - blocks:
        - type: card
          config:
            title: Get started
            description: Run the {{site.base_gateway}} quickstart and enable the AI Proxy plugin.
            icon: /assets/icons/rocket.svg
            cta:
              url: /how-to/get-started-with-ai-gateway/
              align: end
      - blocks:
        - type: card
          config:
            title: Video tutorials
            description: Learn how to use AI plugins with video tutorials.
            icon: /assets/icons/graduation.svg
            cta:
              url: https://konghq.com/products/kong-ai-gateway#videos
              align: end
      - blocks:
        - type: card
          config:
            title: AI plugins
            description: Learn about all the AI plugins.
            icon: /assets/icons/plug.svg
            cta:
              url: /plugins/?category=ai
              align: end
      - blocks:
        - type: card
          config:
            title: AI providers
            description: Learn about the various providers supported by AI Gateway.
            icon: /assets/icons/ai.svg
            cta:
              url: /ai-gateway/ai-providers/
              align: end


  - header:
      type: h2
      text: "AI Gateway capabilities"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    You can enable the AI Gateway features through a set of modern and specialized plugins, using the same model you use for any other {{site.base_gateway}} plugin.
                    When deployed alongside existing {{site.base_gateway}} plugins, {{site.base_gateway}} users can quickly assemble a sophisticated AI management platform without custom code or deploying new and unfamiliar tools.
  - columns:
    - blocks:
      - type: card
        config:
          title: Universal API
          description: Route client requests to various AI providers.
          icon: /assets/icons/plugins/universal-api.svg
          cta:
            url: ./#universal-api
            align: end
    - blocks:
      - type: card
        config:
          title: Rate limiting
          description: Manage traffic to your LLM API.
          icon: /assets/icons/plugins/ai-rate-limiting-advanced.png
          cta:
            url: /plugins/ai-rate-limiting-advanced/
            align: end
    - blocks:
      - type: card
        config:
          title: Semantic caching
          description: Semantically cache responses from LLMs.
          icon: /assets/icons/plugins/ai-semantic-cache.png
          cta:
            url: /plugins/ai-semantic-cache/
            align: end
  - columns:
      - blocks:
        - type: card
          config:
            title: Semantic routing
            description: Semantically distribute requests to different LLM models.
            icon: /assets/icons/plugins/ai-proxy-advanced.png
            cta:
              url: /plugins/ai-proxy-advanced/examples/semantic/
              align: end
      - blocks:
        - type: card
          config:
            title: Request transformations
            description: Use AI to transform requests and responses.
            icon: /assets/icons/plugins/ai-request-transformer.png
            cta:
              url: ./#request-transformations
              align: end
      - blocks:
        - type: card
          config:
            title: Automated RAG injection
            description: Automatically embed RAG logic into your workflows.
            icon: /assets/icons/plugins/ai-rag-injector.png
            cta:
              url: ./#automated-rag
              align: end
  - columns:
      - blocks:
        - type: card
          config:
            title: Data governance
            description: Use AI plugins to control AI data and usage.
            icon: /assets/icons/security.svg
            cta:
              url: ./#data-governance
              align: end
      - blocks:
        - type: card
          config:
            title: Guardrails
            description: Inspect requests and configure content safety and moderation.
            icon: /assets/icons/lock.svg
            cta:
              url: ./#guardrails-and-content-safety
              align: end
      - blocks:
        - type: card
          config:
            title: Prompt engineering
            description: Create prompt templates and manipulate client prompts.
            icon: /assets/icons/code.svg
            cta:
              url: ./#prompt-engineering
              align: end
  - columns:
      - blocks:
        - type: card
          config:
            title: Load balancing
            description: Learn about the load balancing algorithms available for AI Gateway.
            icon: /assets/icons/load-balance.svg
            cta:
              url: ./#load-balancing
              align: end
      - blocks:
        - type: card
          config:
            title: Audit log
            description: Learn about AI Gateway logging capabilities.
            icon: /assets/icons/audit.svg
            cta:
              url: /ai-gateway/ai-audit-log-reference/
              align: end
      - blocks:
        - type: card
          config:
            title: LLM metrics
            description: Expose and visualize LLM metrics.
            icon: /assets/icons/monitor.svg
            cta:
              url: /ai-gateway/monitor-ai-llm-metrics/
              align: end
  - columns:
      - blocks:
        - type: card
          config:
            title: '{{site.konnect_short_name}} Advanced Analytics'
            description: Visualize LLM metrics in {{site.konnect_short_name}}.
            icon: /assets/icons/analytics.svg
            cta:
              url: /advanced-analytics/
              align: end
      - blocks:
        - type: card
          config:
            title: Streaming
            description: Streaming user requests with AI Gateway
            icon: /assets/icons/network.svg
            cta:
              url: /ai-gateway/streaming/
              align: end
      - blocks:
        - type: card
          config:
            title: Secrets management
            description: Use Konnect Config Store to store and reference your LLM provider API keys
            icon: /assets/icons/lock.svg
            cta:
              url: /how-to/configure-the-konnect-config-store/
              align: end


  - header:
    type: h2
  - columns:
    - blocks:
        - type: structured_text
          config:
            header:
              text: "Universal API"
            blocks:
              - type: text
                text: |
                  Kong’s AI Gateway Universal API, delivered through the [AI Proxy](/plugins/ai-proxy/) and [AI Proxy Advanced](/plugins/ai-proxy-advanced/) plugins, simplifies AI model integration by providing a single, standardized interface for interacting with models across multiple providers.

                    - [**Easy to use**](/plugins/ai-proxy/examples/openai-chat-route/): Configure once and access any AI model with minimal integration effort.

                    - [**Load balancing**](/plugins/ai-proxy-advanced/#load-balancing): Automatically distribute AI requests across multiple models or providers for optimal performance and cost efficiency.

                    - [**Retry and fallback**](/plugins/ai-proxy-advanced/#retry-and-fallback): Optimize AI requests based on model performance, cost, or other factors.

                    - [**Cross-plugin integration**](/how-to/visualize-ai-gateway-metrics-with-kibana/): Leverage AI in non-AI API workflows through other Kong Gateway plugins.

    - blocks:
        - type: image
          config:
            url: /assets/images/gateway/universal-api.svg
            alt_text: Overview of AI gateway
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-proxy
      - blocks:
        - type: plugin
          config:
            slug: ai-proxy-advanced

  - header:
    type: h2
  - columns:
      - blocks:
          - type: structured_text
            config:
              header:
                text: "AI Usage Governance"
              blocks:
                - type: text
                  text: |
                    As AI technologies see broader adoption, developers and organizations face new risks—most notably, the risk of sensitive data leaking to AI providers, exposing businesses and their customers to potential breaches and security threats.

                    Managing how data flows to and from AI models has become critical not just for security, but also for compliance and reliability. Without the right controls in place, organizations risk losing visibility into how AI is used across their systems.

      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    <p/><p/>
                    Kong AI Gateway helps mitigate these challenges by offering a suite of plugins that extend beyond basic AI traffic management.

                    - [**Data governance**](./#data-governance): Control how sensitive information is handled and shared with AI models.
                    - [**Prompt engineering**](./#prompt-engineering): Customize and optimize prompts to deliver consistent, high-quality AI outputs.
                    - [**Guardrails and content safety**](./#guardrails-and-content-safety): Enforce policies to prevent inappropriate, unsafe, or non-compliant responses.
                    - [**Automated RAG injection**](./#automated-rag): Seamlessly inject relevant, vetted data into AI prompts without manual RAG implementations.
                    - [**Load balancing**](./#load-balancing): Distribute AI traffic efficiently across multiple model endpoints to ensure performance and reliability.

  - header:
      type: h3
      text: "Data governance"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                      AI Gateway enforces governance on outgoing AI prompts through allow/deny lists, blocking unauthorized requests with 4xx responses. It also provides built-in PII sanitization, automatically detecting and redacting sensitive data across 20+ categories and 12 languages. Running privately and self-hosted for full control and compliance, AI Gateway ensures consistent protection without burdening developers, which helps simplify AI adoption at scale.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-guard
      - blocks:
        - type: plugin
          config:
            slug: ai-semantic-prompt-guard
      - blocks:
        - type: plugin
          config:
            slug: ai-sanitizer

  - header:
      type: h3
      text: "Prompt engineering"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    AI systems are built around prompts, and manipulating those prompts is important for successful adoption of the technologies.
                    Prompt engineering is the methodology of manipulating the linguistic inputs that guide the AI system.
                    Kong AI Gateway supports a set of plugins that allow you to create a simplified and enhanced experience by setting default prompts or manipulating prompts from clients as they pass through the gateway.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-template
      - blocks:
        - type: plugin
          config:
            slug: ai-prompt-decorator

  - header:
      type: h3
      text: "Guardrails and content safety"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    As a platform owner, you may need to moderate all user request content against reputable services to comply with specific sensitive categories when proxying Large Language Model (LLM) traffic. Kong AI Gateway provides built-in capabilities to handle content moderation and ensure content safety, that help you enforce compliance and protect your users across AI-powered applications.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-azure-content-safety
      - blocks:
        - type: card
          config:
            title: Amazon Bedrock guardrails
            description: Include your Amazon Bedrock guardrails configuration in AI Proxy requests
            icon: /assets/icons/bedrock.svg
            cta:
              url: /plugins/ai-proxy/#input-formats
              align: end

  - header:
      type: h3
      text: "Request transformations"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    AI Gateway allows you to use AI technology to augment other API traffic.
                    One example is routing API responses through an AI language translation prompt before returning it to the client.
                    Kong AI Gateway provides two plugins that can be used in conjunction with other upstream API services to weave AI capabilities into API request processing.
                    These plugins can be configured independently of the AI Proxy plugin.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-request-transformer
      - blocks:
        - type: plugin
          config:
            slug: ai-response-transformer


  - header:
      type: h3
      text: "Automated RAG"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    LLMs are only as reliable as the data they can access. When faced with incomplete information, they often produce confident yet incorrect responses—known as “hallucinations.” These hallucinations occur when LLMs lack the necessary domain knowledge.To address this, developers use the **Retrieval-augmented Generation (RAG)** approach, which enriches models with relevant data pulled from vector databases.

                    While standard RAG workflows are resource-heavy, as they require teams to generate embeddings and manually curate them in vector databases, Kong’s **AI RAG Injector** plugin automates this entire process. Instead of embedding RAG logic into every application individually, platform teams can inject vetted data into prompts directly at the gateway layer without any manual interventions.
  - columns:
      - blocks:
        - type: plugin
          config:
            slug: ai-rag-injector

  - header:
      type: h3
      text: "Load balancing"
    columns:
      - blocks:
          - type: structured_text
            config:
              blocks:
                - type: text
                  text: |
                    AI Gateway’s load balancer routes requests across AI models to optimize for speed, cost, and reliability. It supports algorithms like consistent hashing, lowest-latency, usage-based, round-robin, and semantic matching, with built-in retries and fallback for resilience {% new_in 3.10%}. The balancer dynamically selects models based on real-time performance and prompt relevance, and works across mixed environments including OpenAI, Mistral, and Llama models.

  - columns:
      - blocks:
        - type: card
          config:
            title: Load balancing
            description: Learn about the load balancing algorithms available for AI Gateway.
            icon: /assets/icons/load-balance.svg
            cta:
              url: /gateway/load-balancing/#load-balancing-algorithms-for-ai-gateway
              align: end
      - blocks:
        - type: card
          config:
            title: Retry and fallback
            description: Learn about the load balancing algorithms available for AI Gateway.
            icon: /assets/icons/redo.svg
            cta:
              url: /gateway/load-balancing/#load-balancing-algorithms-for-ai-gateway
              align: end

  - header:
      type: h2
      text: How-to Guides

    columns:
      - blocks:
          - type: how_to_list
            config:
              tags:
                - ai
              quantity: 5
              allow_empty: true

  - header:
      text: "Frequently Asked Questions"
      type: h2
    columns:
      - blocks:
        - type: faqs
          config:
            - q: Is AI Gateway available for all deployment modes?
              a: |
                Yes, AI plugins are supported in all [deployment modes](/gateway/deployment-topologies/), including {{site.konnect_short_name}}, self-hosted traditional, hybrid, and DB-less, and on Kubernetes via the [{{site.kic_product_name}}](/kubernetes-ingress-controller/).

            - q: Why should I use AI Gateway instead of adding the LLM's API behind {{site.base_gateway}}?
              a: |
                If you just add an LLM's API behind {{site.base_gateway}}, you can only interact at the API level with internal traffic.
                With AI plugins, {{site.base_gateway}} can understand the prompts that are being sent through the gateway.
                The plugins can inspect the body and provide more specific AI capabilities to your traffic.

providers:
  - name: 'Amazon Bedrock'
    url_pattern: 'https://bedrock-runtime.{region}.amazonaws.com'
    min_version: '3.8'
    chat:
      supported: true
      streaming: true
      upstream_path: 'Uses the <code>Converse</code> and <code>ConverseStream</code> API'
      route_type: 'llm/v1/chat'
      model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
      min_version: '3.8'
    completions:
      supported: true
      streaming: true
      upstream_path: 'Uses the <code>Converse</code> and <code>ConverseStream</code> API'
      route_type: 'llm/v1/completions'
      model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
      min_version: '3.8'
    embeddings:
      supported: true
      streaming: false
      upstream_path: 'Uses the <code>InvokeModel</code> and <code>InvokeWithResponseStream</code> API'
      route_type: 'llm/v1/embeddings'
      model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
      min_version: '3.11'
    batches: # Native format from SDK only
      supported: 'n/a'
      streaming: false
      upstream_path: 'Uses the <code>ModelInvocationJob</code> API'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: ''
      note:
        content: 'Batches processing for Bedrock is supported in the native format from SDK only'
    files:
      supported: 'n/a'
      streaming: false
      upstream_path: '/openai/files'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: ''
      note:
        content: 'Bedrock does not have a dedicated Files API. File storage uses Google Cloud Storage, similar to AWS S3.'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses the <code>InvokeModel</code> API'
        route_type: 'image/v1/images/generations'
        model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: 'Uses the <code>InvokeModel</code> API'
        route_type: 'image/v1/images/edits'
        model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
        min_version: '3.11'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses the <code>StartAsyncInvoke</code> API'
        route_type: 'video/v1/videos/generations'
        model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
        min_version: '3.13'

  - name: 'Anthropic'
    url_pattern: 'https://api.anthropic.com:443/{route_type_path}'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/messages'
      route_type: 'llm/v1/chat'
      model_example: 'claude-3-opus-20240229'
      min_version: '3.6'
    completions:
      supported: true
      streaming: false
      upstream_path: '/v1/complete'
      route_type: 'llm/v1/completions'
      model_example: 'claude-2.1'
      min_version: '3.6'
    batches: # Native format from SDK only
      supported: 'n/a'
      streaming: true
      upstream_path: '/v1/messages/batches'
      route_type: 'files/v1/batches'
      model_example: 'n/a'
      min_version: ''
      note:
        content: 'Batches processing for Antropic is supported in the native format from SDK only'

  - name: 'Azure'
    url_pattern: 'https://{azure_instance}.openai.azure.com:443/openai/deployments/{deployment_name}/{route_type_path}'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/openai/deployments/{deployment_name}/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'gpt-4'
      min_version: '3.6'
    completions:
      supported: true
      streaming: true
      upstream_path: '/openai/deployments/{deployment_name}/completions'
      route_type: 'llm/v1/completions'
      model_example: 'gpt-3.5-turbo-instruct'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/openai/deployments/{deployment_name}/embeddings'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-ada-002<sup>1</sup>'
      min_version: '3.11'
    files:
      supported: true
      streaming: false
      upstream_path: '/openai/files'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: '3.11'
    batches:
      supported: true
      streaming: false
      upstream_path: '/openai/batches'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: '3.11'
    assistants:
      supported: true
      streaming: false
      upstream_path: '/openai/assistants'
      route_type: 'llm/v1/assistants'
      model_example: 'n/a'
      min_version: '3.11'
    responses:
      supported: true
      streaming: false
      upstream_path: '/openai/v1/responses'
      route_type: 'llm/v1/responses'
      model_example: 'n/a'
      min_version: '3.11'
    audio:
      speech:
        supported: true
        streaming: false
        upstream_path: '/openai/audio/speech'
        route_type: 'audio/v1/audio/speech'
        model_example: 'n/a'
        min_version: '3.11'
      transcriptions:
        supported: true
        streaming: false
        upstream_path: '/openai/audio/transcriptions'
        route_type: 'audio/v1/audio/transcriptions'
        model_example: 'n/a'
        min_version: '3.11'
      translations:
        supported: true
        streaming: false
        upstream_path: '/openai/audio/translations'
        route_type: 'audio/v1/audio/translations'
        model_example: 'n/a'
        min_version: '3.11'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: '/openai/images/generations'
        route_type: 'image/v1/images/generations'
        model_example: 'n/a'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: '/openai/images/edits'
        route_type: 'image/v1/images/edits'
        model_example: 'n/a'
        min_version: '3.11'
    realtime:
      supported: true
      streaming: true
      upstream_path: '/openai/realtime'
      route_type: 'realtime/v1/realtime'
      model_example: 'n/a'
      min_version: '3.11'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: '/openai/v1/video/generations/jobs'
        route_type: 'video/v1/videos/generations'
        model_example: 'sora-2'
        min_version: '3.13'

  - name: 'Cerebras'
    url_pattern: 'https://api.cerebras.ai/{route_type_path}'
    min_version: '3.13'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'llama3.1-8b'
      min_version: '3.13'

  - name: 'Cohere'
    url_pattern: 'https://api.cohere.com:443/{route_type_path}'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat'
      route_type: 'llm/v1/chat'
      model_example: 'command'
      min_version: '3.6'
    completions:
      supported: true
      streaming: true
      upstream_path: '/v1/generate'
      route_type: 'llm/v1/completions'
      model_example: 'command'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/v2/embed'
      route_type: 'llm/v1/embeddings'
      model_example: 'embed-english-v3.0'
      min_version: '3.11'

  - name: 'Dashscope'
    url_pattern: 'https://dashscope.aliyuncs.com or https://dashscope-intl.aliyuncs.com'
    min_version: '3.13'
    chat:
      supported: true
      streaming: true
      upstream_path: '/compatible-mode/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'qwen-plus'
      min_version: '3.13'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/compatible-mode/v1/embeddings'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-v1'
      min_version: '3.13'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: '/api/v1/services/aigc/multimodal-generation/generation'
        route_type: 'image/v1/images/generations'
        model_example: 'qwen-image-plus'
        min_version: '3.13'
      edits:
        supported: true
        streaming: false
        upstream_path: '/api/v1/services/aigc/image2image/image-synthesis'
        route_type: 'image/v1/images/edits'
        model_example: 'qwen-image-plus'
        min_version: '3.13'

  - name: 'Gemini'
    url_pattern: 'https://generativelanguage.googleapis.com'
    min_version: '3.8'
    chat:
      supported: true
      streaming: true
      upstream_path: 'Uses <code>generateContent</code> API'
      route_type: 'llm/v1/chat'
      model_example: 'gemini-2.0-flash'
      min_version: '3.8'
    embeddings:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>batchEmbedContents</code> API'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-004'
      min_version: '3.11'
    files: # Native format from SDK only
      supported: 'n/a'
      streaming: false
      upstream_path: 'Uses <code>uploadFile</code> and <code>files</code> API'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: ''
      note:
        content: 'Files processing for Gemini is supported in the native format from SDK only'
    batches: # Native format from SDK only
      supported: 'n/a'
      streaming: false
      upstream_path: 'Uses <code>batches</code> API'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: ''
      note:
        content: 'Batches processing for Gemini is supported in the native format from SDK only'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>generateContent</code> API'
        route_type: 'image/v1/images/generations'
        model_example: 'gemini-2.0-flash-preview-image-generation<sup>1</sup>'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>generateContent</code> API'
        route_type: 'image/v1/images/edits'
        model_example: 'gemini-2.0-flash-preview-image-generation<sup>1</sup>'
        min_version: '3.11'
    realtime: # Native format from SDK only
      supported: true
      streaming: true
      upstream_path: 'Uses <code>BidiGenerateContent</code> API'
      route_type: 'realtime/v1/realtime'
      model_example: 'gemini-live-2.5-flash-preview-native-audio-09-2025'
      min_version: '3.13'
      note:
        content: 'Realtime processing for Gemini is supported in the native format from SDK only'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>predictLongRunning</code> API'
        route_type: 'video/v1/videos/generations'
        model_example: 'veo-3.1-generate-001'
        min_version: '3.13'

  - name: 'Gemini Vertex'
    url_pattern: 'https://aiplatform.googleapis.com/'
    min_version: '3.11'
    chat:
      supported: true
      streaming: true
      upstream_path: 'Uses <code>generateContent</code> API'
      route_type: 'llm/v1/chat'
      model_example: 'gemini-2.0-flash'
      min_version: '3.8'
    completions:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>generateContent</code> API'
      route_type: 'llm/v1/completions'
      model_example: 'gemini-2.0-flash'
      min_version: '3.8'
    embeddings:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>generateContent</code> API'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-004'
      min_version: '3.11'
    files:
      supported: 'n/a'
      streaming: false
      upstream_path: '/openai/files'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: '3.11'
      note:
        content: 'Gemini Vertex does not have a dedicated Files API. File storage uses Google Cloud Storage, similar to AWS S3.'
    batches:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>batchPredictionJobs</code> API'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: '3.13'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>generateContent</code> API'
        route_type: 'image/v1/images/generations'
        model_example: 'gemini-2.0-flash-preview-image-generation<sup>1</sup>'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>generateContent</code> API'
        route_type: 'image/v1/images/edits'
        model_example: 'gemini-2.0-flash-preview-image-generation<sup>1</sup>'
        min_version: '3.11'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>predictLongRunning</code> API'
        route_type: 'video/v1/videos/generations'
        model_example: 'veo-3.1-generate-001'
        min_version: '3.13'

  - name: 'Hugging Face'
    url_pattern: 'https://api-inference.huggingface.co'
    min_version: '3.9'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: '<a href="https://huggingface.co/models?inference=warm&pipeline_tag=text-generation&sort=trending">Use the model name for the specific LLM provider</a>'
      min_version: '3.9'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/hf-inference/models/{model_name}/pipeline/feature-extraction'
      route_type: 'llm/v1/embeddings'
      model_example: '<a href="https://huggingface.co/models?pipeline_tag=feature-extraction">Use the embedding model name</a>'
      min_version: '3.11'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: '/v1/videos'
        route_type: 'video/v1/videos/generations'
        model_example: '<a href="https://huggingface.co/models?pipeline_tag=video-generation">Use the video generation model name</a>'
        min_version: '3.13'

  - name: 'Llama2'
    formats: 'supports Llama2 and Llama3 models and raw, OLLAMA, and OpenAI formats'
    url_pattern: 'As defined in <code>$UPSTREAM_URL</code>'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: 'User-defined'
      route_type: 'llm/v1/chat'
      model_example: 'User-defined'
      min_version: '3.6'
    completions:
      supported: true
      streaming: true
      upstream_path: 'User-defined'
      route_type: 'llm/v1/completions'
      model_example: 'User-defined'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: 'User-defined'
      route_type: 'llm/v1/embeddings'
      model_example: 'User-defined'
      min_version: '3.11'

  - name: 'Mistral'
    formats: 'mistral.ai, OpenAI, raw, and OLLAMA formats'
    url_pattern: 'As defined in <code>$UPSTREAM_URL</code>'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat/completions or user-defined'
      route_type: 'llm/v1/chat'
      model_example: 'mistral-tiny'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/v1/embeddings or user-defined'
      route_type: 'llm/v1/embeddings'
      model_example: 'mistral-embed'
      min_version: '3.11'

  - name: 'OpenAI'
    formats: 'GPT-3.5, GPT-4, GPT-4o, and Multi-Modal'
    url_pattern: 'https://api.openai.com:443/{route_type_path}'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'gpt-4'
      min_version: '3.6'
    completions:
      supported: true
      streaming: true
      upstream_path: '/v1/completions'
      route_type: 'llm/v1/completions'
      model_example: 'gpt-3.5-turbo-instruct'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/v1/embeddings'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-ada-002<sup>1</sup>'
      min_version: '3.11'
    files:
      supported: true
      streaming: false
      upstream_path: '/v1/files'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: '3.11'
    batches:
      supported: true
      streaming: false
      upstream_path: '/v1/batches'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: '3.11'
    assistants:
      supported: true
      streaming: false
      upstream_path: '/v1/assistants'
      route_type: 'llm/v1/assistants'
      model_example: 'gpt-4-1106-preview'
      min_version: '3.11'
    responses:
      supported: true
      streaming: false
      upstream_path: '/v1/responses'
      route_type: 'llm/v1/responses'
      model_example: 'gpt-4-1106-preview'
      min_version: '3.11'
    audio:
      speech:
        supported: true
        streaming: false
        upstream_path: '/v1/audio/speech'
        route_type: 'audio/v1/audio/speech'
        model_example: 'tts-1'
        min_version: '3.11'
      transcriptions:
        supported: true
        streaming: false
        upstream_path: '/v1/audio/transcriptions'
        route_type: 'audio/v1/audio/transcriptions'
        model_example: 'whisper-1'
        min_version: '3.11'
      translations:
        supported: true
        streaming: false
        upstream_path: '/v1/audio/translations'
        route_type: 'audio/v1/audio/translations'
        model_example: 'whisper-1'
        min_version: '3.11'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: '/v1/images/generations'
        route_type: 'image/v1/images/generations'
        model_example: 'dall-e-3'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: '/v1/images/edits'
        route_type: 'image/v1/images/edits'
        model_example: 'dall-e-2'
        min_version: '3.11'
    realtime:
      supported: true
      streaming: true
      upstream_path: '/v1/realtime'
      route_type: 'realtime/v1/realtime'
      model_example: 'gpt-4o'
      min_version: '3.11'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Use the LLM <code>image/generations</code> upstream path'
        route_type: 'video/v1/videos/generations'
        model_example: 'sora-2'
        min_version: '3.13'

  - name: 'xAI'
    url_pattern: 'https://api.x.ai/{route_type_path}'
    min_version: '3.13'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'grok-4-1-fast-reasoning'
      min_version: '3.13'
    responses:
      supported: true
      streaming: false
      upstream_path: '/v1/responses'
      route_type: 'llm/v1/responses'
      model_example: 'grok-4-1-fast-reasoning'
      min_version: '3.13'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: '/v1/images/generations'
        route_type: 'image/v1/images/generations'
        model_example: 'grok-2-image-1212'
        min_version: '3.13'

parameters:
  provider: 'config.targets[].model.provider'
  route_type: 'config.targets.route_type'
  options: 'config.targets[].model.options'
  upstream_url: 'config.targets[].model.options.upstream_url'
  model_name: 'config.targets[].model.name'

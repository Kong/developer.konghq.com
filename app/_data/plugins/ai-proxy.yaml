providers:
  - name: 'Amazon Bedrock'
    url_patterns:
      - 'https://bedrock-runtime.{region}.amazonaws.com'
    min_version: '3.8'
    chat:
      supported: true
      streaming: true
      upstream_path: 'Uses the <code>Converse</code> and <code>ConverseStream</code> API'
      route_type: 'llm/v1/chat'
      model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
      min_version: '3.8'
    completions:
      supported: true
      streaming: true
      upstream_path: 'Uses the <code>Converse</code> and <code>ConverseStream</code> API'
      route_type: 'llm/v1/completions'
      model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
      min_version: '3.8'
    embeddings:
      supported: true
      streaming: false
      upstream_path: 'Uses the <code>InvokeModel</code> and <code>InvokeWithResponseStream</code> API'
      route_type: 'llm/v1/embeddings'
      model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
      min_version: '3.11'
    function_calling:
      supported: true
      streaming: false
      upstream_path: 'Uses the <code>Converse</code> API with tool configuration'
      route_type: 'llm/v1/chat'
      model_example: 'Model-dependent. Supported for Claude, Command, and select models'
      min_version: '3.8'
    batches:
      supported: true
      streaming: false
      upstream_path: 'Uses the <code>ModelInvocationJob</code> API'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: 'n/a'
      note:
        content: 'Batches processing for Bedrock is supported in the native format from SDK only'
    files:
      supported: true
      streaming: false
      upstream_path: '/openai/files'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: 'n/a'
      note:
        content: 'Amazon Bedrock does not have a dedicated files API. File storage uses Google Cloud Storage, similar to AWS S3.'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses the <code>InvokeModel</code> API'
        route_type: 'image/v1/images/generations'
        model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: 'Uses the <code>InvokeModel</code> API'
        route_type: 'image/v1/images/edits'
        model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
        min_version: '3.11'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses the <code>StartAsyncInvoke</code> API'
        route_type: 'video/v1/videos/generations'
        model_example: '<a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html">Use the model name for the specific LLM provider</a>'
        min_version: '3.13'
    native_formats:
      - llm_format: 'bedrock'
        supported_apis:
          - '/model/{model_name}/converse'
          - '/model/{model_name}/converse-stream'
          - '/model/{model_name}/invoke'
          - '/model/{model_name}/invoke-with-response-stream'
          - '/model/{model_name}/retrieveAndGenerate'
          - '/model/{model_name}/retrieveAndGenerateStream'
          - '/model/{model_name}/rerank'
          - '/model/{model_name}/async-invoke'
          - '/model-invocations'
    limitations:
      provider_specific: []
      statistics_logging:
        - 'Statistics logging is not available for image generation or editing APIs for Amazon Bedrock'

  - name: 'Anthropic'
    url_patterns:
      - 'https://api.anthropic.com:443/{route_type_path}'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/messages'
      route_type: 'llm/v1/chat'
      model_example: 'claude-sonnet-4-20250514'
      min_version: '3.6'
    completions:
      supported: true
      streaming: false
      upstream_path: '/v1/complete'
      route_type: 'llm/v1/completions'
      model_example: 'claude-sonnet-4-20250514'
      min_version: '3.6'
    function_calling:
      supported: true
      streaming: false
      upstream_path: '/v1/messages'
      route_type: 'llm/v1/chat'
      model_example: 'claude-sonnet-4-20250514'
      min_version: '3.6'
    batches:
      supported: true
      streaming: true
      upstream_path: '/v1/messages/batches'
      route_type: 'files/v1/batches'
      model_example: 'n/a'
      min_version: 'n/a'
      note:
        content: 'Batches processing for Anthropic is supported in the native format from SDK only'
    native_formats:
      - llm_format: 'anthropic'
        supported_apis:
          - '/v1/messages'
          - '/v1/messages/batches'
    limitations:
      provider_specific:
        - 'Does not support `llm/v1/completions` or `llm/v1/embeddings`'
      statistics_logging:
        - 'No statistics logging for `llm/v1/completions`'

  - name: 'Azure'
    url_patterns:
      - 'https://{azure_instance}.openai.azure.com:443/openai/deployments/{deployment_name}/{route_type_path}'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/openai/deployments/{deployment_name}/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'gpt-4o'
      min_version: '3.6'
    completions:
      supported: true
      streaming: true
      upstream_path: '/openai/deployments/{deployment_name}/completions'
      route_type: 'llm/v1/completions'
      model_example: 'gpt-4o-mini'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/openai/deployments/{deployment_name}/embeddings'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-3-small'
      min_version: '3.11'
      note:
        content: 'Use `text-embedding-3-small` or `text-embedding-3-large` for dynamic dimensions.'
    function_calling:
      supported: true
      streaming: false
      upstream_path: '/openai/deployments/{deployment_name}/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'gpt-4o'
      min_version: '3.6'
    files:
      supported: true
      streaming: false
      upstream_path: '/openai/files'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: '3.11'
    batches:
      supported: true
      streaming: false
      upstream_path: '/openai/batches'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: '3.11'
    assistants:
      supported: true
      streaming: false
      upstream_path: '/openai/assistants'
      route_type: 'llm/v1/assistants'
      model_example: 'n/a'
      min_version: '3.11'
      note:
        content: 'Assistans API requires header <code>OpenAI-Beta: assistants=v2</code>'
    responses:
      supported: true
      streaming: false
      upstream_path: '/openai/v1/responses'
      route_type: 'llm/v1/responses'
      model_example: 'n/a'
      min_version: '3.11'
      note:
        content: 'Responses API requires <code>config.azure_api_version</code> set to <code>"preview"</code>'
    audio:
      speech:
        supported: true
        streaming: false
        upstream_path: '/openai/audio/speech'
        route_type: 'audio/v1/audio/speech'
        model_example: 'n/a'
        min_version: '3.11'
      transcriptions:
        supported: true
        streaming: false
        upstream_path: '/openai/audio/transcriptions'
        route_type: 'audio/v1/audio/transcriptions'
        model_example: 'n/a'
        min_version: '3.11'
      translations:
        supported: true
        streaming: false
        upstream_path: '/openai/audio/translations'
        route_type: 'audio/v1/audio/translations'
        model_example: 'n/a'
        min_version: '3.11'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: '/openai/images/generations'
        route_type: 'image/v1/images/generations'
        model_example: 'n/a'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: '/openai/images/edits'
        route_type: 'image/v1/images/edits'
        model_example: 'n/a'
        min_version: '3.11'
    realtime:
      supported: true
      streaming: true
      upstream_path: '/openai/realtime'
      route_type: 'realtime/v1/realtime'
      model_example: 'n/a'
      min_version: '3.11'
      note:
        content: 'For requests to Azure OpenAI realtime API, include include the header `OpenAI-Beta: realtime=v1`.'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: '/openai/v1/video/generations/jobs'
        route_type: 'video/v1/videos/generations'
        model_example: 'sora-2'
        min_version: '3.13'
    limitations:
      provider_specific: []
      statistics_logging:
        - 'No statistics logging for assistants, batch, or audio APIs'

  - name: 'Cerebras'
    url_patterns:
      - 'https://api.cerebras.ai/{route_type_path}'
    min_version: '3.13'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'llama-3.3-70b'
      min_version: '3.13'
    limitations:
      provider_specific: []
      statistics_logging: []

  - name: 'Cohere'
    url_patterns:
      - 'https://api.cohere.com:443/{route_type_path}'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat'
      route_type: 'llm/v1/chat'
      model_example: 'command-a-03-2025'
      min_version: '3.6'
    completions:
      supported: true
      streaming: true
      upstream_path: '/v1/generate'
      route_type: 'llm/v1/completions'
      model_example: 'command-r-plus-08-2024'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/v2/embed'
      route_type: 'llm/v1/embeddings'
      model_example: 'embed-english-v3.0'
      min_version: '3.11'
    function_calling:
      supported: true
      streaming: false
      upstream_path: '/v1/chat'
      route_type: 'llm/v1/chat'
      model_example: 'command-a-03-2025'
      min_version: '3.6'
    native_formats:
      - llm_format: 'cohere'
        supported_apis:
          - '/v1/rerank'
          - '/v2/rerank'
    limitations:
      provider_specific: []
      statistics_logging: []

  - name: 'Dashscope'
    url_patterns:
      - 'https://dashscope.aliyuncs.com'
      - 'https://dashscope-intl.aliyuncs.com'
    min_version: '3.13'
    chat:
      supported: true
      streaming: true
      upstream_path: '/compatible-mode/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'qwen-plus'
      min_version: '3.13'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/compatible-mode/v1/embeddings'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-v1'
      min_version: '3.13'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: '/api/v1/services/aigc/multimodal-generation/generation'
        route_type: 'image/v1/images/generations'
        model_example: 'qwen-image-plus'
        min_version: '3.13'
      edits:
        supported: true
        streaming: false
        upstream_path: '/api/v1/services/aigc/image2image/image-synthesis'
        route_type: 'image/v1/images/edits'
        model_example: 'qwen-image-plus'
        min_version: '3.13'
    limitations:
      provider_specific: []
      statistics_logging: []

  - name: 'Gemini'
    url_patterns:
      - 'https://generativelanguage.googleapis.com'
    min_version: '3.8'
    chat:
      supported: true
      streaming: true
      upstream_path: 'Uses <code>generateContent</code> API'
      route_type: 'llm/v1/chat'
      model_example: 'gemini-2.5-flash'
      min_version: '3.8'
    embeddings:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>batchEmbedContents</code> API'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-004'
      min_version: '3.11'
    function_calling:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>generateContent</code> API with function declarations'
      route_type: 'llm/v1/chat'
      model_example: 'gemini-2.5-flash'
      min_version: '3.8'
    files:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>uploadFile</code> and <code>files</code> API'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: 'n/a'
      note:
        content: 'Files processing for Gemini is supported in the native format from SDK only'
    batches:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>batches</code> API'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: 'n/a'
      note:
        content: 'Batches processing for Gemini is supported in the native format from SDK only'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>generateContent</code> API'
        route_type: 'image/v1/images/generations'
        model_example: 'gemini-2.5-flash-preview-image-generation'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>generateContent</code> API'
        route_type: 'image/v1/images/edits'
        model_example: 'gemini-2.5-flash-preview-image-generation'
        min_version: '3.11'
    realtime:
      supported: true
      streaming: true
      upstream_path: 'Uses <code>BidiGenerateContent</code> API'
      route_type: 'realtime/v1/realtime'
      model_example: 'gemini-2.5-flash-preview-native-audio'
      min_version: '3.13'
      note:
        content: 'Realtime processing for Gemini is supported in the native format from SDK only'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>predictLongRunning</code> API'
        route_type: 'video/v1/videos/generations'
        model_example: 'veo-3.1-generate-001'
        min_version: '3.13'
    native_formats:
      - llm_format: 'gemini'
        supported_apis:
          - '/v1beta/models/{model_name}:generateContent'
          - '/v1beta/models/{model_name}:streamGenerateContent'
          - '/v1beta/models/{model_name}:embedContent'
          - '/v1beta/models/{model_name}:batchEmbedContent'
          - '/v1beta/batches'
          - '/upload/{file_id}/files'
          - '/v1beta/files'
    limitations:
      provider_specific:
        - 'Gemini only supports `auth.allow_override = false`'
      statistics_logging: []

  - name: 'Gemini Vertex'
    url_patterns:
      - 'https://aiplatform.googleapis.com/'
    min_version: '3.11'
    chat:
      supported: true
      streaming: true
      upstream_path: 'Uses <code>generateContent</code> API'
      route_type: 'llm/v1/chat'
      model_example: 'gemini-2.5-flash'
      min_version: '3.8'
    completions:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>generateContent</code> API'
      route_type: 'llm/v1/completions'
      model_example: 'gemini-2.5-flash'
      min_version: '3.8'
    embeddings:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>generateContent</code> API'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-004'
      min_version: '3.11'
    function_calling:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>generateContent</code> API with function declarations'
      route_type: 'llm/v1/chat'
      model_example: 'gemini-2.5-flash'
      min_version: '3.8'
    files:
      supported: true
      streaming: false
      upstream_path: '/openai/files'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: '3.11'
      note:
        content: 'Gemini Vertex does not have a dedicated Files API. File storage uses Google Cloud Storage, similar to AWS S3.'
    batches:
      supported: true
      streaming: false
      upstream_path: 'Uses <code>batchPredictionJobs</code> API'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: '3.13'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>generateContent</code> API'
        route_type: 'image/v1/images/generations'
        model_example: 'gemini-2.5-flash-preview-image-generation'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>generateContent</code> API'
        route_type: 'image/v1/images/edits'
        model_example: 'gemini-2.5-flash-preview-image-generation'
        min_version: '3.11'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Uses <code>predictLongRunning</code> API'
        route_type: 'video/v1/videos/generations'
        model_example: 'veo-3.1-generate-001'
        min_version: '3.13'
    native_formats:
      - llm_format: 'gemini'
        supported_apis:
          - '/v1/projects/{project_id}/locations/{location}/models/{model_name}:generateContent'
          - '/v1/projects/{project_id}/locations/{location}/models/{model_name}:streamGenerateContent'
          - '/v1/projects/{project_id}/locations/{location}/models/{model_name}:embedContent'
          - '/v1/projects/{project_id}/locations/{location}/models/{model_name}:batchEmbedContent'
          - '/v1/projects/{project_id}/locations/{location}/models/{model_name}:predictLongRunning'
          - '/v1/projects/{project_id}/locations/{location}/rankingConfigs/{config_name}:rank'
          - '/v1/projects/{project_id}/locations/{location}/batchPredictionJobs'
    limitations:
      provider_specific: []
      statistics_logging: []

  - name: 'Hugging Face'
    url_patterns:
      - 'https://api-inference.huggingface.co'
    min_version: '3.9'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: '<a href="https://huggingface.co/models?inference=warm&pipeline_tag=text-generation&sort=trending">Use the model name for the specific LLM provider</a>'
      min_version: '3.9'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/hf-inference/models/{model_name}/pipeline/feature-extraction'
      route_type: 'llm/v1/embeddings'
      model_example: '<a href="https://huggingface.co/models?pipeline_tag=feature-extraction">Use the embedding model name</a>'
      min_version: '3.11'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: '/v1/videos'
        route_type: 'video/v1/videos/generations'
        model_example: '<a href="https://huggingface.co/models?pipeline_tag=video-generation">Use the video generation model name</a>'
        min_version: '3.13'
    native_formats:
      - llm_format: 'huggingface'
        supported_apis:
          - '/generate'
          - '/generate_stream'
    limitations:
      provider_specific: []
      statistics_logging: []

  - name: 'Llama2'
    formats: 'supports Llama2 and Llama3 models and raw, OLLAMA, and OpenAI formats'
    url_patterns:
      - '$UPSTREAM_URL'
    url_is_variable: true
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: 'User-defined'
      route_type: 'llm/v1/chat'
      model_example: 'User-defined'
      min_version: '3.6'
    completions:
      supported: true
      streaming: true
      upstream_path: 'User-defined'
      route_type: 'llm/v1/completions'
      model_example: 'User-defined'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: 'User-defined'
      route_type: 'llm/v1/embeddings'
      model_example: 'User-defined'
      min_version: '3.11'
    limitations:
      provider_specific:
        - 'Raw format lacks support for `llm/v1/embeddings`'
      statistics_logging: []

  - name: 'Mistral'
    formats: 'mistral.ai, OpenAI, raw, and OLLAMA formats'
    url_patterns:
      - '$UPSTREAM_URL'
    url_is_variable: true
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat/completions or user-defined'
      route_type: 'llm/v1/chat'
      model_example: 'mistral-large-latest'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/v1/embeddings or user-defined'
      route_type: 'llm/v1/embeddings'
      model_example: 'mistral-embed'
      min_version: '3.11'
    function_calling:
      supported: true
      streaming: false
      upstream_path: '/v1/chat/completions or user-defined'
      route_type: 'llm/v1/chat'
      model_example: 'mistral-large-latest'
      min_version: '3.6'
    limitations:
      provider_specific: []
      statistics_logging: []

  - name: 'OpenAI'
    formats: 'GPT-4o, GPT-4.1, and Multi-Modal'
    url_patterns:
      - 'https://api.openai.com:443/{route_type_path}'
    min_version: '3.6'
    chat:
      supported: true
      streaming: true
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'gpt-4o'
      min_version: '3.6'
    completions:
      supported: true
      streaming: true
      upstream_path: '/v1/completions'
      route_type: 'llm/v1/completions'
      model_example: 'gpt-4o-mini'
      min_version: '3.6'
    embeddings:
      supported: true
      streaming: false
      upstream_path: '/v1/embeddings'
      route_type: 'llm/v1/embeddings'
      model_example: 'text-embedding-3-small'
      min_version: '3.11'
      note:
        content: 'Use `text-embedding-3-small` or `text-embedding-3-large` for dynamic dimensions.'
    function_calling:
      supported: true
      streaming: false
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'gpt-4o'
      min_version: '3.6'
    files:
      supported: true
      streaming: false
      upstream_path: '/v1/files'
      route_type: 'llm/v1/files'
      model_example: 'n/a'
      min_version: '3.11'
    batches:
      supported: true
      streaming: false
      upstream_path: '/v1/batches'
      route_type: 'llm/v1/batches'
      model_example: 'n/a'
      min_version: '3.11'
    assistants:
      supported: true
      streaming: false
      upstream_path: '/v1/assistants'
      route_type: 'llm/v1/assistants'
      model_example: 'gpt-4o'
      min_version: '3.11'
      note:
        content: 'Requires header <code>OpenAI-Beta: assistants=v2</code>'
    responses:
      supported: true
      streaming: false
      upstream_path: '/v1/responses'
      route_type: 'llm/v1/responses'
      model_example: 'gpt-4o'
      min_version: '3.11'
    audio:
      speech:
        supported: true
        streaming: false
        upstream_path: '/v1/audio/speech'
        route_type: 'audio/v1/audio/speech'
        model_example: 'tts-1'
        min_version: '3.11'
      transcriptions:
        supported: true
        streaming: false
        upstream_path: '/v1/audio/transcriptions'
        route_type: 'audio/v1/audio/transcriptions'
        model_example: 'whisper-1'
        min_version: '3.11'
      translations:
        supported: true
        streaming: false
        upstream_path: '/v1/audio/translations'
        route_type: 'audio/v1/audio/translations'
        model_example: 'whisper-1'
        min_version: '3.11'
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: '/v1/images/generations'
        route_type: 'image/v1/images/generations'
        model_example: 'dall-e-3'
        min_version: '3.11'
      edits:
        supported: true
        streaming: false
        upstream_path: '/v1/images/edits'
        route_type: 'image/v1/images/edits'
        model_example: 'dall-e-2'
        min_version: '3.11'
    realtime:
      supported: true
      streaming: true
      upstream_path: '/v1/realtime'
      route_type: 'realtime/v1/realtime'
      model_example: 'gpt-4o-realtime-preview'
      min_version: '3.11'
      note:
        content: 'For requests to OpenAI realtime API, include include the header `OpenAI-Beta: realtime=v1`.'
    video:
      generations:
        supported: true
        streaming: false
        upstream_path: 'Use the LLM <code>image/generations</code> upstream path'
        route_type: 'video/v1/videos/generations'
        model_example: 'sora-2'
        min_version: '3.13'
    limitations:
      provider_specific: []
      statistics_logging:
        - 'No statistics logging for assistants, batch, or audio APIs'

  - name: 'xAI'
    url_patterns:
      - 'https://api.x.ai:443/{route_type_path}'
    min_version: '3.13'
    chat:
      supported: true
      streaming: false
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'grok-3'
      min_version: '3.13'
    completions:
      supported: false
      streaming: false
    embeddings:
      supported: false
      streaming: false
    function_calling:
      supported: true
      streaming: false
      upstream_path: '/v1/chat/completions'
      route_type: 'llm/v1/chat'
      model_example: 'grok-3'
      min_version: '3.13'
    files:
      supported: false
      streaming: false
    batches:
      supported: false
      streaming: false
    assistants:
      supported: false
      streaming: false
    responses:
      supported: true
      streaming: false
      upstream_path: '/v1/responses'
      route_type: 'llm/v1/responses'
      model_example: 'grok-3'
      min_version: '3.13'
    audio:
      speech:
        supported: false
        streaming: false
      transcriptions:
        supported: false
        streaming: false
      translations:
        supported: false
        streaming: false
    image:
      generations:
        supported: true
        streaming: false
        upstream_path: '/v1/images/generations'
        route_type: 'image/v1/images/generations'
        model_example: 'grok-2-image'
        min_version: '3.13'
      edits:
        supported: false
        streaming: false
    realtime:
      supported: false
      streaming: false
    limitations:
      provider_specific: []
      statistics_logging: []

parameters:
  provider: 'config.model.provider'
  route_type: 'config.route_type'
  options: 'config.model.options'
  upstream_url: 'config.model.options.upstream_url'
  model_name: 'config.model.name'
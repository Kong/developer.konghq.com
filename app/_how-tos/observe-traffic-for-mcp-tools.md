---
title: Observe MCP traffic for autogenerated MCP tools
content_type: how_to
related_resources:
  - text: AI Gateway
    url: /ai-gateway/
  - text: AI MCP
    url: /plugins/ai-mcp/
  - text: Prometheus plugin
    url: /plugins/prometheus/
  - text: A trust and control layer for proxying traffic to MCP servers
    url: /mcp/

description: Learn how to monitor traffic for autogenerated MCP tools using the AI MCP plugin and Prometheus, so you can track tool usage and latency.

products:
  - gateway
  - ai-gateway

permalink: /mcp/observe-traffic-for-mcp-tools/

series:
  id: mcp-conversion
  position: 2

works_on:
  - on-prem

min_version:
  gateway: '3.12'

plugins:
  - ai-mcp
  - prometheus

entities:
  - service
  - route
  - plugin

tags:
  - ai
  - openai
  - mcp
  - serverless

tldr:
  q: How do I monitor autogenerated MCP tool traffic?
  a: |
    Reconfigure the AI MCP plugin to enable logging of payloads and statistics for your MCP tools, then enable the Prometheus plugin to scrape and collect these metrics for monitoring.

tools:
  - deck

cleanup:
  inline:
    - title: Prometheus
      content: |
        Once you are done experimenting with Prometheus, you can use the following
        commands to stop the Prometheus server you created in this guide:

        ```sh
        docker stop kong-quickstart-prometheus
        ```
      icon_url: /assets/icons/third-party/prometheus.svg
    - title: Destroy the {{site.base_gateway}} container
      include_content: cleanup/products/gateway
      icon_url: /assets/icons/gateway.svg

automated_tests: false
---

## Reconfigure the AI MCP plugin

To observe traffic for MCP tools, you first must **enable logging and statistics** on the AI MCP plugin. Apply the below configuration for the AI MCP plugin with enabled logging capabilities:

{% entity_examples %}
entities:
  plugins:
    - name: ai-mcp
      route: mcp-route
      config:
        logging:
          log_payloads: true
          log_statistics: true
        tools:
        - description: Get users
          method: GET
          path: /marketplace/users
          parameters:
            - name: id
              in: query
              required: false
              schema:
                type: string
              description: Optional user ID
        - description: Get orders for a user
          method: GET
          path: /marketplace/orders
          parameters:
            - description: User ID to filter orders
              in: query
              name: userid
              required: true
              schema:
                type: string
        server:
          timeout: 60000
{% endentity_examples %}

## Enable the Prometheus plugin

Before you configure Prometheus, enable the [Prometheus plugin](/plugins/prometheus/) on {{site.base_gateway}}:

{% entity_examples %}
entities:
  plugins:
    - name: prometheus
      config:
        status_code_metrics: true
        ai_metrics: true
{% endentity_examples %}

## Configure Prometheus

Create a `prometheus.yml` file:

```sh
touch prometheus.yml
```

Now, add the following to the `prometheus.yml` file to configure Prometheus to scrape MCP traffic metrics:

```yaml
scrape_configs:
 - job_name: 'kong'
   scrape_interval: 5s
   static_configs:
     - targets: ['kong-quickstart-gateway:8001']
```
{: data-deployment-topology="on-prem" }


Run a Prometheus server, and pass it the configuration file created in the previous step:

```sh
docker run -d --name kong-quickstart-prometheus \
  --network=kong-quickstart-net -p 9090:9090 \
  -v $(PWD)/prometheus.yml:/etc/prometheus/prometheus.yml \
  prom/prometheus:latest
```

Prometheus will begin to scrape metrics data from {{site.base_gateway}}.

## Validate the configuration

You can validate that the Prometheus plugin is collecting metrics by generating MCP traffic to the `mcp-service`. Enter the following question in the Cursor chat:

```text
What users do you see in the API?
```

Once Cursor agent has finished reasoning, run the following to query the collected `kong_ai_mcp_latency_ms` metric data:

```
curl -s 'localhost:9090/api/v1/query?query=kong_ai_mcp_latency_ms_bucket'
```

This should return something like the following:

```json
{
  "status": "success",
  "data": {
    "resultType": "vector",
    "result": [
      {
        "metric": {
          "__name__": "kong_ai_mcp_latency_ms_bucket",
          "instance": "kong-quickstart-gateway:8001",
          "job": "kong",
          "le": "25.0",
          "method": "tools/call",
          "route": "mcp-route",
          "service": "mcp-service",
          "tool_name": "mcp-route-1",
          "workspace": "default"
        },
        "value": [1755759385.507, "3"]
      },
      {
        "metric": {
          "__name__": "kong_ai_mcp_latency_ms_bucket",
          "instance": "kong-quickstart-gateway:8001",
          "job": "kong",
          "le": "25.0",
          "method": "tools/call",
          "route": "mcp-route",
          "service": "mcp-service",
          "tool_name": "mcp-route-2",
          "workspace": "default"
        },
        "value": [1755759385.507, "2"],
      "..."
    ]
  }
}
```

---
title: Log MCP traffic for autogenerated MCP Weather API tools
content_type: how_to
related_resources:
  - text: AI Gateway
    url: /ai-gateway/
  - text: AI MCP Proxy
    url: /plugins/ai-mcp-proxy/
  - text: HTTP Long
    url: /plugins/http-log/

description: |
  Enable logging in the AI MCP Proxy plugin to capture MCP tool calls, then use the HTTP Log plugin to record and inspect the payloads and responses from the WeatherAPI tool.

products:
  - gateway
  - ai-gateway

permalink: /mcp/observe-autogenerated-mcp-tools-for-weather-api/

series:
  id: mcp-weather-api
  position: 2

works_on:
  - on-prem

min_version:
  gateway: '3.12'

plugins:
  - ai-mcp
  - http-log

entities:
  - service
  - route
  - plugin

tags:
  - ai
  - mcp

tldr:
  q: How do I observe and validate MCP traffic from autogenerated tools?
  a: |
    Use the AI MCP Proxy plugin to expose the upstream WeatherAPI as an MCP tool, then use the HTTP Log plugin to capture tool calls and validate their payloads and responses.

tools:
  - deck

cleanup:
  inline:
    - title: Destroy the {{site.base_gateway}} container
      include_content: cleanup/products/gateway
      icon_url: /assets/icons/gateway.svg

automated_tests: false
---

## Reconfigure the AI MCP Proxy plugin

We can move on to configuring the AI MCP Proxy plugin. This setup exposes the upstream WeatherAPI endpoint as an MCP tool, enabling our AI client, Cursor, to call it directly.

In this configuration, we also define the tool along with its parameters—including the configured API key—so that the MCP client can make tool calls for our weather queries.

{% entity_examples %}
entities:
  plugins:
    - name: ai-mcp-proxy
      route: weather-route
      config:
        logging:
          log_payloads: true
          log_statistics: true
        mode: conversion-listener
        tools:
        - description: Get current weather for a location
          method: GET
          path: "/weather"
          parameters:
          - name: key
            in: query
            description: Your API key
            required: true
            schema:
              type: string
          - name: q
            in: query
            required: true
            schema:
              type: string
            description: Location query. Accepts US Zipcode, UK Postcode, Canada Postalcode,
              IP address, latitude/longitude, or city name.
        server:
          timeout: 60000
{% endentity_examples %}


## Log MCP traffic

Before we send tool calls, we need to set up the HTTP Logs plugin to check how many tokens we've managed to save by using our configuration. First, create an HTTP logs plugin:

{% entity_examples%}
entities:
  plugins:
    - name: http-log
      service: weather-service
      config:
        http_endpoint: http://host.docker.internal:9999/
        headers:
          Authorization: Bearer some-token
        method: POST
        timeout: 3000
{% endentity_examples%}

Let's run a simple log collector script which collect logs at `9999` port. Copy and run this snippet in your terminal:

```
cat <<EOF > log_server.py
from http.server import BaseHTTPRequestHandler, HTTPServer
import datetime

LOG_FILE = "kong_logs.txt"

class LogHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        timestamp = datetime.datetime.now().isoformat()

        content_length = int(self.headers['Content-Length'])
        post_data = self.rfile.read(content_length).decode('utf-8')

        log_entry = f"{timestamp} - {post_data}\n"
        with open(LOG_FILE, "a") as f:
            f.write(log_entry)

        print("="*60)
        print(f"Received POST request at {timestamp}")
        print(f"Path: {self.path}")
        print("Headers:")
        for header, value in self.headers.items():
            print(f"  {header}: {value}")
        print("Body:")
        print(post_data)
        print("="*60)

        # Send OK response
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"OK")

if __name__ == '__main__':
    server_address = ('', 9999)
    httpd = HTTPServer(server_address, LogHandler)
    print("Starting log server on http://0.0.0.0:9999")
    httpd.serve_forever()
EOF
```

Now, run this script with Python:

```sh
python3 log_server.py
```

If script is successful, you'll receive the following prompt in your terminal:

```sh
Starting log server on http://0.0.0.0:9999
```

## Validate MCP tools configuration

You can validate that the Prometheus plugin is collecting metrics by generating MCP traffic to the `weather-service`. Enter the following question in the Cursor chat:

```text
What is the current weather in London?
```

Once Cursor agent has finished reasoning, you will see the following MCP audit log entries logged by the HTTP plugin in your terminal:


```json
{
  "..."
  "ai": {
    "mcp": {
      "mcp_session_id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
      "rpc": [
        {
          "tool_name": "weather-route-1",
          "id": "19",
          "latency": 200,
          "response_body_size": 1053,
          "payload": {
            "request": "{\"params\":{\"name\":\"weather-route-1\",\"arguments\":{\"query_key\":\"02e7c45e34024e6ca7e52559251908\",\"query_q\":\"New York\"},\"_meta\":{\"progressToken\":19}},\"id\":19,\"jsonrpc\":\"2.0\",\"method\":\"tools/call\"}",
            "response": "{\"result\":{\"isError\":false,\"content\":[{\"type\":\"text\",\"text\":\"{\\\"location\\\":{\\\"name\\\":\\\"New York\\\",\\\"region\\\":\\\"New York\\\",\\\"country\\\":\\\"United States of America\\\",\\\"lat\\\":40.7142,\\\"lon\\\":-74.0064,\\\"tz_id\\\":\\\"America/New_York\\\",\\\"localtime_epoch\\\":1755764969,\\\"localtime\\\":\\\"2025-08-21 04:29\\\"},\\\"current\\\":{\\\"temp_c\\\":15.4,\\\"temp_f\\\":59.7,\\\"condition\\\":{\\\"text\\\":\\\"Light rain\\\"}}}]}},\"id\":19,\"jsonrpc\":\"2.0\"}"
          },
          "method": "tools/call"
        }
      ]
    }
  }
}
```
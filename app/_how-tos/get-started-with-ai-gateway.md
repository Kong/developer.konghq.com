---
title: Get started with AI Gateway
content_type: how_to

products:
    - gateway

works_on:
    - on-prem
    - konnect

plugins:
  - ai-proxy

entities: 
  - service
  - route
  - plugin

tags:
    - get-started
    - ai-gateway

tldr: 
  q: What is AI Gateway, and how can I get started with it?
  a: |
    With Kong's AI Gateway, you can deploy AI infrastructure for traffic 
    that is sent to one or more LLMs. This lets you semantically route, secure, observe, accelerate, 
    and govern traffic using a special set of AI plugins that are bundled with {{site.base_gateway}} distributions.

    This tutorial will help you get started with AI Gateway by setting up the AI Proxy plugin with OpenAI. 

    {:.info}
    > **Note:**
    > This quickstart runs a Docker container to explore {{ site.base_gateway }}'s capabilities. 
    If you want to run {{ site.base_gateway }} as a part of a production-ready API platform, start with the [Install](/gateway/install/) page.

tools:
    - deck
  
prereqs:
  inline:
    - title: OpenAI
      content: |
        This tutorial uses the AI Proxy plugin with OpenAI. You'll need to [create an OpenAI account](https://auth.openai.com/create-account) and [get an API key](https://platform.openai.com/api-keys). Once you have your API key, create an environment variable:

        ```sh
        export OPENAI_KEY='<api-key>'
        ```

cleanup:
  inline:
    - title: Clean up Konnect environment
      include_content: cleanup/platform/konnect
      icon_url: /assets/icons/gateway.svg
    - title: Destroy the {{site.base_gateway}} container
      include_content: cleanup/products/gateway
      icon_url: /assets/icons/gateway.svg

min_version:
    gateway: '3.6'

next_steps:
  - text: Set up load balancing using AI Proxy Advanced plugin
    url: /plugins/ai-proxy-advanced/
  - text: Cache traffic using the AI Semantic cache plugin
    url: /plugins/ai-semantic-cache/
  - text: Secure traffic with the AI Prompt Guard
    url: /plugins/ai-prompt-guard/
  - text: Provide prompt templates with AI Prompt Template
    url: /plugins/ai-prompt-template/
  - text: Programmatically inject system or assistant prompts to all incoming prompts with the AI Prompt Decorator
    url: /plugins/ai-prompt-decorator/
  - text: Learn about all the AI plugins
    url: /plugins/?category=ai

---

## 1. Check that {{site.base_gateway}} is running

{% include how-tos/steps/ping-gateway.md %}


## 2. Create a Gateway Service

Create a Service to contain the Route for the LLM provider:

{% entity_examples %}
entities:
    services:
    - name: llm-service
      url: http://localhost:32000
{% endentity_examples %}

The URL can point to any empty host, as it won't be used by the plugin.

## 3. Create a Route

Create a Route for the LLM provider. In this example we're creating a chat route, so we'll use `/chat` as the path:

{% entity_examples %}
entities:
    routes:
    - name: openai-chat
      service:
        name: llm-service
      paths:
      - /chat
{% endentity_examples %}

## 4. Enable the AI Proxy plugin

Enable the AI Proxy plugin to create a chat route:

{% entity_examples %}
entities:
    plugins:
    - name: ai-proxy
      config:
        route_type: "llm/v1/chat"
        model:
          provider: "openai"
{% endentity_examples %}

In this example, we're setting up the plugin with minimal configuration, which means:
* The client is allowed to use any model in the `openai` provider and must provide the model name in the request body.
* The client must provide an `Authorization` header with an OpenAI API key.

If needed, you can restrict the models that can be consumed by specifying the model name explicitly using the [`config.model.name`](/plugins/ai-proxy/reference/#schema--config-model-name) parameter. 

You can also provide the OpenAI API key directly in the configuration with the [`config.auth.header_name`](/plugins/ai-proxy/reference/#schema--config-model-name) and [`config.auth.header_value`](/plugins/ai-proxy/reference/#schema--config-auth-header-value) parameters so that the client doesnâ€™t have to send them.

## 5. Validate

To validate, you can send a `POST` request to the `/chat` endpoint, using the correct [input format](/plugins/ai-proxy/#input-formats).
Since we didn't add the model name and API key in the plugin configuration, make sure to include them in the request:

{% validation request-check %}
url: /chat
status_code: 200
method: POST
headers:
    - 'Accept: application/json'
    - 'Content-Type: application/json'
    - 'Authorization: Bearer $OPENAI_KEY'
body:
  model: gpt-4
  messages:
  - role: "user"
    content: "Say this is a test!"
{% endvalidation %}

You should get a `200 OK` response, and the response body should contain `This is a test`.

---
title: "LLM usage reporting"
content_type: reference
layout: reference
description: | 
    {{site.observability}} allows you to monitor and optimize your LLM usage by providing detailed insights into objects such as token consumption, costs, and latency.

breadcrumbs:
  - /observability/

min_version:
  gateway: '3.8'
products:
    - gateway
    - observability
    - ai-gateway
tags:
  - analytics
  - ai
works_on:
    - konnect
api_specs:
    - konnect/analytics-requests
related_resources:
  - text: "{{site.konnect_short_name}} {{site.observability}}"
    url: /observability/
  - text: Explorer Reference
    url: /observability/explorer/
schema:
    api: konnect/analytics-requests
faqs:
  - q: What data can I collect LLM data from?
    a: |
        * **Application**
        * **Cache Status**
        * **Consumer**
        * **Control Plane**
        * **Control Plane Group**
        * **Embeddings Model**
        * **Embeddings Provider**
        * **Provider**
        * **Request Model**
        * **Response Model**
        * **Route**
  - q: What can I do after customizing an Explorer dashboard?
    a: |
        * **Save as a Report**: This function creates a new custom report based on your current view, allowing you to revisit these specific insights at a later time.
        * **Export as CSV**: If you prefer to analyze your data using other tools, you can download the current view as a CSV file, making it portable and ready for further analysis elsewhere.    
---

{{site.observability}} allows you to monitor and optimize your LLM usage by providing detailed insights into objects such as token consumption, costs, and latency. 

With LLM usage reporting, you can:

* Track token consumption: Monitor the number of tokens processed by the different LLM models you have configured. 
* Understand costs: Gain visibility into the costs associated with your LLM providers. 
* Measure latency: Analyze the latency involved in processing LLM requests. 

To use this feature, navigate to the [Explorer dashboard](https://cloud.konghq.com/us/analytics/explorer) and switch between API usage and LLM usage using the dataset dropdown. Metrics and groupings will dynamically adjust based on the selected dataset. 

## Metrics

Traffic metrics provide insight into which of your services are being used and how they are responding. Within a single report, you have the flexibility to choose one or multiple metrics from the same category.

<!--vale off-->
{% table %}
columns:
  - title: "Attribute"
    key: "attribute"
  - title: "Unit"
    key: "unit"
  - title: "Description"
    key: "description"
rows:
  - attribute: "Completion Tokens"
    unit: "Count"
    description: "Completion tokens are any tokens that the model generates in response to an input."
  - attribute: "Prompt Tokens"
    unit: "Count"
    description: "Prompt tokens are the number of tokens in the prompt that are input into the model."
  - attribute: "Total Tokens"
    unit: "Count"
    description: "Sum of all tokens used in a single request to the model. It includes both the tokens in the input (prompt) and the tokens generated by the model (completion)."
  - attribute: "Time per Tokens"
    unit: "Number"
    description: "Average time in milliseconds to generate a token. Calculated as LLM latency divided by the number of tokens."
  - attribute: "Costs"
    unit: "Cost"
    description: "Represents the resulting costs for a request. Final costs = (total number of prompt tokens × input cost per token) + (total number of completion tokens × output cost per token) + (total number of prompt tokens × embedding cost per token)."
  - attribute: "Response Model"
    unit: "String"
    description: "Represents which AI model was used to process the prompt by the AI provider."
  - attribute: "Request Model"
    unit: "String"
    description: "Represents which AI model was used to process the prompt."
  - attribute: "Provider Name"
    unit: "String"
    description: "Represents which AI provider was used to process the prompt."
  - attribute: "Plugin ID"
    unit: "String"
    description: "Represents the UUID of the plugin."
  - attribute: "LLM Latency"
    unit: "Latency"
    description: "Total time taken to receive a full response after a request sent from Kong (LLM latency + connection time)."
  - attribute: "Embeddings Latency"
    unit: "Latency"
    description: "Time taken to generate the vector for the prompt string."
  - attribute: "Fetch Latency"
    unit: "Latency"
    description: "Total time taken to return a cache."
  - attribute: "Cache Status"
    unit: "String"
    description: "Shows if the response comes directly from the upstream or not. Possible values: `hit` or `Miss`."
  - attribute: "Embeddings Model"
    unit: "String"
    description: "AI providers may have multiple embedding models. This represents the model used for the embeddings."
  - attribute: "Embeddings Provider"
    unit: "String"
    description: "Provider used for generating embeddings."
  - attribute: "Embeddings Token"
    unit: "Count"
    description: "Tokens input into the model for embeddings."
  - attribute: "Embeddings Cost"
    unit: "Cost"
    description: "Cost of caching."
  - attribute: "Cost Savings"
    unit: "Cost"
    description: "Cost savings from cache."
{% endtable %}
<!--vale on-->

## Time intervals

{% include_cached /konnect/analytics-intervals.md %}

{
  "3.13.0.0": [
    {
      "message": "Added dimension configuration support for Amazon Titan Embed v2 models.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Added support for Cerebras - a new AI Provider.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added support for HuggingFace's new serverless API in the AI Proxy plugin, enabling seamless integration and improved compatibility.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added support for native format video generation in Bedrock and Gemini LLM drivers, allowing direct passthrough of provider-specific video generation requests without format conversion.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added support for batch API of bedrock.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Introduced Alibaba Dashscope as new provider.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added support for batch mode of anthropic.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added support for inline batch mode of gemini\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where cohere embedding model on Bedrock returned a bad request error\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where `tls_certificate_verify` configuration was not respected in JSON-RPC and AWS STS calls.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Added tool_calls passthrough and preserved finish_reason in Huggingface driver responses.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where HuggingFace embedding driver were incorrectly parsed responses from embedding API\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where extra inputs were not permitted for huggingface inference provider\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where using ai-proxy-advanced in conjunction with a logging plugin (such as file-log) resulting in missing information on the last entry of the balancer \"tries\" section.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.12.0.0": [
    {
      "message": "Added `time_to_first_token` and `request_mode` for observability.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the Anthropic provider mapped the wrong `stop_reason` to Chat and Completions responses.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the Anthropic provider failed to stream function call responses.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the Anthropic provider could truncate tokens in streaming responses.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where OpenAI chat completion's `tool_choice` was not converted to Anthropic's.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the `model` field was incorrect in Gemini responses when a variable was used as the model name.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the `model` field was missing in OpenAI-format Gemini responses in some situations.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the Gemini provider did not properly handle multiple tool calls across different turns.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where an empty array was encoded as an empty object in structured output.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AWS stream parser didn't parse correctly when the frame was incomplete.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where JSON array data wasn't correctly parsed.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue with Azure Managed Identity credentials causing improper request balancing.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AWS Bedrock did not properly handle image editing requests through the `image/v1/images/edits` route.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AWS Bedrock streaming requests would panic when the ai-semantic-cache plugin was enabled.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the resource was not being passed correctly when using Azure as the provider.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.12.0.1": [
    {
      "message": "Added dimension configuration support for Amazon Titan Embed v2 models.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed integration with HuggingFace by adding support for the new serverless API, ensuring seamless compatibility.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where HuggingFace embedding driver were incorrectly parsed responses from embedding API\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where extra inputs were not permitted for huggingface inference provider\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.12.0.2": [
    {
      "message": "Fixed an issue where cohere embedding model on Bedrock returned a bad request error\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.0": [
    {
      "message": "Fixed an issue where large request payload was not logged.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where latency metric was not implemented for streaming responses.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where SSE terminator may not have correct ending characters.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the response body for observability may be larger than the real one because of the stale data.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "ai-proxy, ai-proxy-advanced: Deprecated the `preserve` route_type. You are encouraged to use new route_types added in version 3.11.x.x and onwards.",
      "type": "deprecation",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where some of ai metrics was missed in analytics",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed an issue where patterns using multiple capture groups (e.g., `$(group1)/$(group2)`) failed to extract expected matches.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where model name did not escape when using AWS Bedrock inference profile.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced would use corrupted plugin config.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy returns 403 using gemini provider.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Implemented a faster SSE parser.",
      "type": "performance",
      "scope": "Plugin"
    },
    {
      "message": "If any [AI Gateway plugin](/plugins/?category=ai) has been enabled in a self-managed Kong Gateway deployment for more than a week, upgrades from 3.10 versions to 3.11.0.0 will fail due to a license migration issue. This does not affect Konnect deployments.\n\nA fix will be provided in 3.11.0.1.\n\nSee [breaking changes in 3.11](/gateway/breaking-changes/#known-issues-in-3-11-0-0) for a temporary workaround.",
      "type": "known-issues",
      "scope": "Plugin"
    }
  ],
  "3.11.0.1": [
    {
      "message": "Fixed an issue where the llm license migration could fail if the license counter contained more than one week of data.",
      "scope": "Plugin",
      "type": "bugfix"
    }
  ],
  "3.11.0.2": [
    {
      "message": "Fixed an issue where aws stream parser didn't parse correctly when the frame was incomplete.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AWS Bedrock streaming requests would panic when the ai-semantic-cache plugin was enabled.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced can't properly failover to a Bedrock target.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced might produce duplicate content in the response when the SSE event was truncated.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced might drop content in the response when the SSE event was truncated.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where resource was not being passed correctly when using Azure as provider.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.3": [
    {
      "message": "Fixed an issue where the Anthropic provider mapped the wrong stop_reason to Chat and Completions responses.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where JSON array data is not correctly parsed.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue with Azure Managed Identity credentials causing improper request balancing.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AWS Bedrock did not properly handle image editing requests through the `image/v1/images/edits` route.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fix panic in LLM observability when populating token usage for AI Proxy token usage details\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.4": [
    {
      "message": "Fixed an issue when empty array in structured output was encoded as empty object.",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.5": [
    {
      "message": "Fixed an issue where OpenAI chat completion's tool_choice was not converted to Anthropic's.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the `model` field was incorrect in Gemini responses when a variable was used as the model name.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where `model` field was missing in OpenAI format response from Gemini provider in some situations.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed integration with HuggingFace by adding support for the new serverless API, ensuring seamless compatibility.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where extra inputs were not permitted for huggingface inference provider\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.6": [
    {
      "message": "Fixed an issue where HuggingFace embedding driver were incorrectly parsed responses from embedding API\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.0": [
    {
      "message": "Deprecated `preserve` mode in `config.route_type`. Use `config.llm_format` instead. The `preserve` mode setting will be removed in a future release.",
      "type": "deprecation",
      "scope": "Plugin"
    },
    {
      "message": "Changed the serialized log key of AI metrics from `ai.ai-proxy` to `ai.proxy` to avoid conflicts with metrics generated from plugins other than AI Proxy and AI Proxy Advanced. If you are using logging plugins (for example, File Log, HTTP Log, etc.), you will have to update metrics pipeline configurations to reflect this change.\n",
      "type": "Breaking Change",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug in the Azure provider where `model.options.upstream_path` overrides would always return a 404 response.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where Azure streaming responses would be missing individual tokens.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where response streaming in Gemini and Bedrock providers was returning whole chat responses in one chunk.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug with the Gemini provider, where multimodal requests (in OpenAI format) would not transform properly.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI upstream URL trailing would be empty.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Added support for boto3 SDKs for Bedrock provider, and for Google GenAI SDKs for Gemini provider.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Allow authentication to Bedrock services with assume roles in AWS.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where Gemini streaming responses were getting truncated and/or missing tokens.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an incorrect error thrown when trying to log streaming responses.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "- Fixed an issue where templates weren't being resolved correctly.\n- The plugins now support nested fields.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed tool calls not working in streaming mode for Bedrock and Gemini providers.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed preserve mode.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Some active tracing latency values are incorrectly reported as having zero length when using the AI Proxy plugin.\n",
      "type": "known-issues",
      "scope": "Plugin"
    }
  ],
  "3.10.0.1": [
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced would use corrupted plugin config.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.3": [
    {
      "message": "Fixed an issue where large request payload was not logged.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where SSE terminator may not have correct ending characters.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the response body for observability may be larger than the real one because of the stale data.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where patterns using multiple capture groups (e.g., `$(group1)/$(group2)`) failed to extract expected matches.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy returns 403 using gemini provider.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.4": [
    {
      "message": "Fixed an issue where aws stream parser didn't parse correctly when the frame was incomplete.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AWS Bedrock streaming requests would panic when the ai-semantic-cache plugin was enabled.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where model name did not escape when using AWS Bedrock inference profile.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced can't properly failover to a Bedrock target.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where resource was not being passed correctly when using Azure as provider.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.6": [
    {
      "message": "Fixed an issue where OpenAI chat completion's tool_choice was not converted to Anthropic's.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where JSON array data is not correctly parsed.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue with Azure Managed Identity credentials causing improper request balancing.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.7": [
    {
      "message": "Fixed integration with HuggingFace by adding support for the new serverless API, ensuring seamless compatibility.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where HuggingFace embedding driver were incorrectly parsed responses from embedding API\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where extra inputs were not permitted for huggingface inference provider\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.1.1": [
    {
      "message": "Fixed issue of template not being resolved correctly and supported nested fields.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed preserve mode.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.1.0": [
    {
      "message": "Fixed Gemini streaming responses getting truncated and/or missing tokens.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed incorrect error thrown when trying to log streaming responses.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed tool calls not working in streaming mode for Bedrock and Gemini providers.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.1.2": [
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced would use corrupted plugin config.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.0.0": [
    {
      "message": "Fixed a bug where tools (function) calls to Anthropic would return empty results.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where tools (function) calls to Bedrock would return empty results.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where Bedrock Guardrail config was ignored.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where tools (function) calls to Cohere would return empty results.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where Gemini provider would return an error if content safety failed in AI Proxy.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where tools (function) calls to Gemini (or via Vertex) would return empty results.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Transformer plugins always returned a 404 error when using 'Google One' Gemini subscriptions.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Disabled HTTP/2 ALPN handshake for connections on routes configured with AI-proxy. \n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed issue where multi-modal requests is blocked on azure provider. \n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.0.1": [
    {
      "message": "Fixed a bug in the Azure provider where `model.options.upstream_path` overrides would always return 404.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where Azure streaming responses would be missing individual tokens.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where response streaming in Gemini and Bedrock providers was returning whole chat responses in one chunk.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where multimodal requests (in OpenAI format) would not transform properly, when using the Gemini provider.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Reverted the analytics container key from \"proxy\" to \"ai-proxy\" to align with previous versions.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.8.1.0": [
    {
      "message": "Fixed an issue where AI Transformer plugins always returned a 404 error when using 'Google One' Gemini subscriptions.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed issue where multi-modal requests is blocked on azure provider. \n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.8.0.0": [
    {
      "message": "AI plugins: retrieved latency data and pushed it to logs and metrics.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "allow AI plugin to read request from buffered file",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Add `allow_override` option to allow overriding the upstream model auth parameter or header from the caller's request.\n",
      "scope": "Plugin",
      "type": "feature"
    },
    {
      "message": "Kong AI Gateway (AI Proxy and associated plugin family) now supports \nall AWS Bedrock \"Converse API\" models.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where certain Azure models would return partial tokens/words \nwhen in response-streaming mode.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed a bug where Cohere and Anthropic providers don't read the `model` parameter properly \nfrom the caller's request body.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed a bug where using \"OpenAI Function\" inference requests would log a \nrequest error, and then hang until timeout.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed a bug where AI Proxy would still allow callers to specify their own model,  \nignoring the plugin-configured model name.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed a bug where AI Proxy would not take precedence of the \nplugin's configured model tuning options, over those in the user's LLM request.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Kong AI Gateway (AI Proxy and associated plugin family) now supports \nthe Google Gemini \"chat\" (generateContent) interface.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Allowed mistral provider to use mistral.ai managed service by omitting upstream_url",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added a new response header X-Kong-LLM-Model that displays the name of the language model used in the AI-Proxy plugin.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where setting OpenAI SDK model parameter \"null\" caused analytics \nto not be written to the logging plugin(s).\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed issue when response is gzipped even if client doesn't accept.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed certain AI plugins cannot be applied per consumer or per service.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Resolved a bug where the object constructor would set data on the class instead of the instance",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue for multi-modal inputs are not properly validated and calculated.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed a bug where Azure Managed-Identity tokens would never rotate  \nin case of a network failure when authenticating.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.7.1.3": [
    {
      "message": "Fixed a bug where certain Azure models would return partial tokens/words \nwhen in response-streaming mode.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed a bug where Cohere and Anthropic providers don't read the `model` parameter properly \nfrom the caller's request body.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed a bug where using \"OpenAI Function\" inference requests would log a \nrequest error, and then hang until timeout.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed a bug where AI Proxy would still allow callers to specify their own model,  \nignoring the plugin-configured model name.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed a bug where AI Proxy would not take precedence of the \nplugin's configured model tuning options, over those in the user's LLM request.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed a bug where setting OpenAI SDK model parameter \"null\" caused analytics \nto not be written to the logging plugin(s).\n",
      "scope": "Plugin",
      "type": "bugfix"
    }
  ],
  "3.7.1.0": [
    {
      "message": "Resolved a bug where the object constructor would set data on the class instead of the instance",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.7.0": [
    {
      "message": "To support the new messages API of `Anthropic`, the upstream path of the `Anthropic` for `llm/v1/chat` route type has changed from `/v1/complete` to `/v1/messages`.\n",
      "type": "Breaking Change",
      "scope": "Plugin",
      "jiras": [
        "FTI-5770"
      ]
    },
    {
      "message": "AI Proxy now reads most prompt tuning parameters from the client,\nwhile the plugin config parameters under `model_options` are now just defaults.\nThis fixes support for using the respective provider's native SDK.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "AI Proxy now has a `preserve` option for `route_type`, where the requests and responses\nare passed directly to the upstream LLM. This is to enable compatibility with any\nand all models and SDKs that may be used when calling the AI services.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed the bug that the `route_type` `/llm/v1/chat` didn't include the analytics in the responses.\n",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Added support for streaming event-by-event responses back to the client on supported providers.\n",
      "scope": "Plugin",
      "type": "feature"
    }
  ],
  "3.7.0.0": [
    {
      "message": "Added support for Managed Identity authentication when using the\nAzure provider with AI Proxy.\n",
      "type": "feature",
      "scope": "Plugin"
    }
  ],
  "3.6.0.0": [
    {
      "message": "Introduced the new **AI Proxy** plugin that enables simplified integration with various AI provider Large Language Models.",
      "type": "feature",
      "scope": "Plugin"
    }
  ]
}
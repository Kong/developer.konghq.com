title: 'OpenAI SDK: Proxy multiple models deployed in the same Azure instance'
description: Configure one route to proxy multiple models deployed in the same Azure instance.
extended_description: |
  Configure one route to proxy multiple models deployed in the same Azure instance.

  When you apply this configuration, you can set the SDK endpoint to `http://localhost:8000/azure`. When the Azure instance parameter is set to `my-gpt-3-5`, the Python SDK produces the URL `http://localhost:8000/openai/deployments/my-gpt-3-5/chat/completions` and is directed to the respective Azure deployment ID and model.

    {:.warning}
    > For this configuration to work properly, you need a Gateway Route with the following configuration:
    > ```
    > routes:
    >  - name: azure-chat
    >    paths:
    >      - "~/openai/deployments/(?<azure_instance>[^#?/]+)/chat/completions"
    >    methods:
    >      - POST
    > ```

weight: 103

requirements:
- Cohere account
- Mistral account

config:
  route_type: "llm/v1/chat"
  auth:
    header_name: "api-key"
    header_value: ${azure_key}
  logging:
    log_statistics: true
    log_payloads: false
  model:
    provider: "azure"
    name: "$(uri_captures.azure_instance)"
    options:
      azure_instance: "my-openai-instace"
      azure_deployment_id: "$(uri_captures.azure_instance)"

variables:
  azure_key:
    value: $AZURE_API_KEY
    description: The API key used to authenticate requests to Azure.

tools:
  - deck
  - admin-api
  - konnect-api
  - kic
  - terraform

group: open-ai

title: 'Configure AI Proxy for Claude Code with HuggingFace'
description: 'Set up the AI Proxy plugin to work with Claude Code, using HuggingFace Inference API as the LLM provider with Llama models.'

extended_description: |
  {% new_in 3.13 %} Set up the AI Proxy plugin to work with Claude Code, using HuggingFace Inference API as the LLM provider with Llama 3.3 70B model.
  For a detailed guide on how to use HuggingFace with Claude Code see [this guide](/how-to/use-claude-code-with-ai-gateway-huggingface/)

show_in_api: true
weight: 901

requirements:
- HuggingFace account with API access
- HuggingFace API token

config:
  llm_format: anthropic
  route_type: llm/v1/chat
  logging:
    log_payloads: false
    log_statistics: true
  auth:
    header_name: Authorization
    header_value: Bearer ${huggingface_token}
  model:
    provider: huggingface
    name: meta-llama/Llama-3.3-70B-Instruct

variables:
  huggingface_token:
    value: $HUGGINGFACE_API_TOKEN
    description: The API token to use to connect to HuggingFace Inference API. Obtain this from your HuggingFace account settings.

tools:
  - deck
  - admin-api
  - konnect-api
  - kic
  - terraform

group: claude-code
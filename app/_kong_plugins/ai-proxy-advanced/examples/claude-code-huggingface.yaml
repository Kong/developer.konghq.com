title: 'Configure AI Proxy Advanced for Claude Code with HuggingFace'
description: 'Set up the AI Proxy Advanced plugin to work with Claude Code, using HuggingFace Inference API as the LLM provider.'

extended_description: |
  {% new_in 3.13 %} Set up the AI Proxy Advanced plugin to work with Claude Code, using HuggingFace Inference API as the LLM provider with Llama 3.3 70B model.
  For a detailed guide on how to use HuggingFace with Claude Code see [this-guide](/how-to/use-claude-code-with-ai-gateway-huggingface/)

show_in_api: true
weight: 906

requirements:
- HuggingFace account with API access
- HuggingFace API token

config:
  llm_format: anthropic
  targets:
    - route_type: llm/v1/chat
      logging:
        log_statistics: true
        log_payloads: false
      auth:
        header_name: Authorization
        header_value: Bearer ${huggingface_token}
      model:
        provider: huggingface
        name: meta-llama/Llama-3.3-70B-Instruct

variables:
  huggingface_token:
    value: $HUGGINGFACE_API_TOKEN
    description: The API token to use to connect to HuggingFace Inference API. Obtain this from your HuggingFace account settings.

tools:
  - deck
  - admin-api
  - konnect-api
  - kic
  - terraform

group: claude-code
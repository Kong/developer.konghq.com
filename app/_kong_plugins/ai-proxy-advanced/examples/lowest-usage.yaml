
title: 'Load balancing: Lowest-usage'
description: 'Configure the plugin to use two OpenAI models and route requests based on the number of tokens in the prompt.'
extended_description: |
  Configure the plugin to use two OpenAI models and route requests based on the number of tokens in the prompt.

  The lowest-usage algorithm distributes requests to the model with the lowest usage volume. By default, the usage is calculated based on the total number of tokens in the prompt and in the response. However, you can customize this using the [`config.balancer.tokens_count_strategy`](/plugins/ai-proxy-advanced/reference/#schema--config-balancer-tokens-count-strategy) parameter. You can use:
    * `prompt-tokens` to only count the tokens in the prompt
    * `completion-tokens` to only count the tokens in the response
    * `total-tokens` to count both tokens in the prompt and in the response
    * {% new_in 3.10 %} `cost` to count the cost of the tokens.<br/> You must set the `cost` parameter in each model configuration to use this strategy and `log_statistics` must be enabled.


weight: 110

requirements:
  - An OpenAI account

config:
  balancer:
    algorithm: lowest-usage
    tokens_count_strategy: prompt-tokens
  targets:
  - model:
      name: gpt-4
      provider: openai
      options:
        max_tokens: 512
        temperature: 1.0
    route_type: llm/v1/chat
    auth:
      header_name: Authorization
      header_value: Bearer ${key}
  - model:
      name: gpt-4o-mini
      provider: openai
      options:
        max_tokens: 512
        temperature: 1.0
    route_type: llm/v1/chat
    auth:
      header_name: Authorization
      header_value: Bearer ${key}

variables:
  key:
    value: $OPENAI_API_KEY
    description: The API key to use to connect to OpenAI.

tools:
  - deck
  - admin-api
  - konnect-api
  - kic
  - terraform

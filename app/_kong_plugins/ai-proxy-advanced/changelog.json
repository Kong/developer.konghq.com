{
  "3.11.0.0": [
    {
      "message": "Add `tried_targets` field in serialized analytics logs for record of all tried ai targets.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "ai-proxy, ai-proxy-advanced: Deprecated the `preserve` route_type. You are encouraged to use new route_types added in version 3.11.x.x and onwards.",
      "type": "deprecation",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where some of ai metrics was missed in analytics",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed an issue where AI Proxy Advanced can't failover from other provider to Bedrock.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fix consistent-hashing algorithm not using correct value to hash.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the stale semantic key vector may not be refreshed after the plugin config is updated.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced would use corrupted plugin config.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.0": [
    {
      "message": "Deprecated `preserve` mode in `config.route_type`. Use `config.llm_format` instead. The `preserve` mode setting will be removed in a future release.",
      "type": "deprecation",
      "scope": "Plugin"
    },
    {
      "message": "Changed the serialized log key of AI metrics from `ai.ai-proxy` to `ai.proxy` to avoid conflicts with metrics generated from plugins other than AI Proxy and AI Proxy Advanced. If you are using logging plugins (for example, File Log, HTTP Log, etc.), you will have to update metrics pipeline configurations to reflect this change.\n",
      "type": "Breaking Change",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI upstream URL trailing would be empty.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Added support for boto3 SDKs for Bedrock provider, and for Google GenAI SDKs for Gemini provider.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added new `priority` balancer algorithm, which allows setting apriority group for each upstream model.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added the `failover_criteria` configuration option, which allows retrying requests to the next upstream server in case of failure.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added cost to `tokens_count_strategy` when using the lowest-usage load balancing strategy.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added the `huggingface`, `azure`, `vertex`, and `bedrock` providers to embeddings. They can be used by the ai-proxy-advanced, ai-semantic-cache, ai-semantic-prompt-guard, and ai-rag-injector plugins.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Allow authentication to Bedrock services with assume roles in AWS.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added the ability to set a catch-all target in semantic routing.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the ai-proxy-advanced plugin failed to failover between providers of different formats.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the ai-proxy-advanced plugin identity running failed in retry scenarios.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.1": [
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced would use corrupted plugin config.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.0.0": [
    {
      "message": "Added support for streaming responses to the AI Proxy Advanced plugin.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Made the\n`embeddings.model.name` config field a free text entry, enabling use of a\nself-hosted (or otherwise compatible) model.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where stale plugin config was not updated in dbless and hybrid mode.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where lowest-usage and lowest-latency strategy did not update data points correctly.",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.8.1.0": [
    {
      "message": "Fixed an issue where stale plugin config was not updated in dbless and hybrid mode.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where lowest-usage and lowest-latency strategy did not update data points correctly.",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.8.0.0": [
    {
      "message": "allow AI plugin to read request from buffered file",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added the `ai-proxy-advanced` plugin that supports advanced load balancing between LLM services.",
      "type": "feature",
      "scope": "Plugin"
    }
  ]
}
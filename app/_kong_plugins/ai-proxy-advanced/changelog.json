{
  "3.13.0.0": [
    {
      "message": "Supported health checks and circuit breaker for the load balancer. Added two new fields `max_fails` and `fail_timeout`.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added missing `least-connections` load balancing algorithm to the AI Proxy Advanced plugin which was missed in the previous release.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed balancer retry failures caused by expired DNS entries by preloading DNS for targets.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Added support for Gemini live websocket in the ai-proxy-advanced plugin.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added load balance, failover and circuit breaker feature for semantic routing.",
      "scope": "Plugin",
      "type": "feature"
    },
    {
      "message": "Fixed an issue where the token count for Gemini Vertex embeddings API in native format was incorrect.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the token count for Gemini Vertex embeddings API in OpenAI format was incorrect.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed intermittent 500 responses from the AI Proxy Advanced plugin when using Azure OpenAI\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the native format option did not work correctly for non-openai formats.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the semantic load balancing with pgvector namespace was not functioning correctly.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue when using responses API and background mode.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where Files content analytics extraction is not handled properly for Azure.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where requests to Anthropic Claude models via Azure Foundry were not being processed correctly.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AWS Bedrock invoke command is not properly proxied.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the Gemini image generation model responses were not being processed correctly.",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed missing `id` and `created` fields in certain drivers.",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed an issue where the `max_completion_tokens` parameter was not being set correctly for O1 series models (e.g., `o1`, `o3`, `o4`, `gpt-5`).",
      "scope": "Plugin",
      "type": "bugfix"
    }
  ],
  "3.13.0.1": [
    {
      "message": "Fixed an issue where GCP tokens would not be proactively refreshed before expiration.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.12.0.0": [
    {
      "message": "Added latency and cost observability to realtime API.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the AI Proxy Advanced plugin's lowest latency load balancer could fail to select a target when only one target was available, leading to 500 errors.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the ai-retry phase was not correctly setting the namespace in the `kong.plugin.ctx`, causing the ai-proxy-advanced balancer to retry the first target more than once.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the Gemini provider didn't support Model Garden.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where managed identity could be cached incorrectly.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where Mistral models would return `Unsupported field: seed` when using some inference libraries.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where array input wasn't being properly validated for certain providers. Note that Bedrock, Gemini Public, and Mistral providers don't support array input, as before.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where some Titan embeddings models reported malformed requests.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Added support for Gemini Rerank in native format.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Exposed `input_tokens_details.cached_tokens_details.image_tokens_count` in observability metrics.",
      "type": "feature",
      "scope": "Plugin"
    }
  ],
  "3.12.0.1": [
    {
      "message": "Fixed an issue where the token count for Gemini Vertex embeddings API in native format was incorrect.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the token count for Gemini Vertex embeddings API in OpenAI format was incorrect.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the native format option did not work correctly for non-openai formats.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the semantic load balancing with pgvector namespace was not functioning correctly.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AWS Bedrock invoke command is not properly proxied.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.12.0.2": [
    {
      "message": "Fixed intermittent 500 responses from the AI Proxy Advanced plugin when using Azure OpenAI\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where Files content analytics extraction is not handled properly for Azure.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.12.0.3": [
    {
      "message": "Fixed balancer retry failures caused by expired DNS entries by preloading DNS for targets.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed excessive DNS warning logs for template variable endpoints like `$(headers.x_gcp_endpoint)`. These warnings appeared on every configuration change because template variables cannot be resolved until request time.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where metrics were missing and v2 API was unavailable in Cohere native format.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where GCP tokens would not be proactively refreshed before expiration.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.0": [
    {
      "message": "Add `tried_targets` field in serialized analytics logs for record of all tried ai targets.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "ai-proxy, ai-proxy-advanced: Deprecated the `preserve` route_type. You are encouraged to use new route_types added in version 3.11.x.x and onwards.",
      "type": "deprecation",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where some of ai metrics was missed in analytics",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed an issue where AI Proxy Advanced can't failover from other provider to Bedrock.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fix consistent-hashing algorithm not using correct value to hash.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the stale semantic key vector may not be refreshed after the plugin config is updated.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced would use corrupted plugin config.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "If any [AI Gateway plugin](/plugins/?category=ai) has been enabled in a self-managed Kong Gateway deployment for more than a week, upgrades from 3.10 versions to 3.11.0.0 will fail due to a license migration issue. This does not affect Konnect deployments.\n\nA fix will be provided in 3.11.0.1.\n\nSee [breaking changes in 3.11](/gateway/breaking-changes/#known-issues-in-3-11-0-0) for a temporary workaround.",
      "type": "known-issues",
      "scope": "Plugin"
    }
  ],
  "3.11.0.1": [
    {
      "message": "Fixed an issue where the llm license migration could fail if the license counter contained more than one week of data.",
      "scope": "Plugin",
      "type": "bugfix"
    }
  ],
  "3.11.0.2": [
    {
      "message": "Fixed an issue where ai-retry phase was not correctly setting the namespace in the kong.plugin.ctx, causing ai-proxy-advanced balancer retry first target more than once.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced can't properly failover to a Bedrock target.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced might produce duplicate content in the response when the SSE event was truncated.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced might drop content in the response when the SSE event was truncated.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where managed identity may not be cached properly.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.3": [
    {
      "message": "Fix panic in LLM observability when populating token usage for AI Proxy token usage details\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "AI Proxy Advanced: Fixed an issue where gemini provider not support model garden",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where Mistral models would return `Unsupported field: seed` when using some inference libraries.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Skip unknown cached details key from o11y observation.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where array input is not recognized as valid request for some providers. Bedrock, Gemini Public and Mistral provider don't accept array input as before.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where some Titan embeddings model reported malformed request. \n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Support for Gemini Rerank in native format",
      "type": "feature",
      "scope": "Plugin"
    }
  ],
  "3.11.0.7": [
    {
      "message": "Fixed balancer retry failures caused by expired DNS entries by preloading DNS for targets.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the token count for Gemini Vertex embeddings API in native format was incorrect.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the token count for Gemini Vertex embeddings API in OpenAI format was incorrect.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed excessive DNS warning logs for template variable endpoints like `$(headers.x_gcp_endpoint)`. These warnings appeared on every configuration change because template variables cannot be resolved until request time.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed intermittent 500 responses from the AI Proxy Advanced plugin when using Azure OpenAI\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the native format option did not work correctly for non-openai formats.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where Files content analytics extraction is not handled properly for Azure.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AWS Bedrock invoke command is not properly proxied.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed missing `id` and `created` fields in certain drivers.",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed an issue where the `max_completion_tokens` parameter was not being set correctly for O1 series models (e.g., `o1`, `o3`, `o4`, `gpt-5`).",
      "scope": "Plugin",
      "type": "bugfix"
    }
  ],
  "3.10.0.0": [
    {
      "message": "Deprecated `preserve` mode in `config.route_type`. Use `config.llm_format` instead. The `preserve` mode setting will be removed in a future release.",
      "type": "deprecation",
      "scope": "Plugin"
    },
    {
      "message": "Changed the serialized log key of AI metrics from `ai.ai-proxy` to `ai.proxy` to avoid conflicts with metrics generated from plugins other than AI Proxy and AI Proxy Advanced. If you are using logging plugins (for example, File Log, HTTP Log, etc.), you will have to update metrics pipeline configurations to reflect this change.\n",
      "type": "Breaking Change",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI upstream URL trailing would be empty.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Added support for boto3 SDKs for Bedrock provider, and for Google GenAI SDKs for Gemini provider.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added new `priority` balancer algorithm, which allows setting apriority group for each upstream model.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added the `failover_criteria` configuration option, which allows retrying requests to the next upstream server in case of failure.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added cost to `tokens_count_strategy` when using the lowest-usage load balancing strategy.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added the `huggingface`, `azure`, `vertex`, and `bedrock` providers to embeddings. They can be used by the ai-proxy-advanced, ai-semantic-cache, ai-semantic-prompt-guard, and ai-rag-injector plugins.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Allow authentication to Bedrock services with assume roles in AWS.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added the ability to set a catch-all target in semantic routing.",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the ai-proxy-advanced plugin failed to failover between providers of different formats.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the ai-proxy-advanced plugin identity running failed in retry scenarios.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.1": [
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced would use corrupted plugin config.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.3": [
    {
      "message": "Fixed an issue where the stale semantic key vector may not be refreshed after the plugin config is updated.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.4": [
    {
      "message": "Fixed an issue where the Gemini provider could not use Anthropic 'rawPredict' endpoint models hosted in Vertex.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where ai-retry phase was not correctly setting the namespace in the kong.plugin.ctx, causing ai-proxy-advanced balancer retry first target more than once.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced can't properly failover to a Bedrock target.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fix consistent-hashing algorithm not using correct value to hash.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.6": [
    {
      "message": "Fixed an issue where AI Proxy Advanced can't failover from other provider to Bedrock.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed model name escape or bedrock from request \n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.1.2": [
    {
      "message": "Fixed an issue where AI Proxy and AI Proxy Advanced would use corrupted plugin config.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.0.0": [
    {
      "message": "Added support for streaming responses to the AI Proxy Advanced plugin.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Made the\n`embeddings.model.name` config field a free text entry, enabling use of a\nself-hosted (or otherwise compatible) model.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where stale plugin config was not updated in dbless and hybrid mode.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where lowest-usage and lowest-latency strategy did not update data points correctly.",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.8.1.0": [
    {
      "message": "Fixed an issue where stale plugin config was not updated in dbless and hybrid mode.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where lowest-usage and lowest-latency strategy did not update data points correctly.",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.8.0.0": [
    {
      "message": "allow AI plugin to read request from buffered file",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added the `ai-proxy-advanced` plugin that supports advanced load balancing between LLM services.",
      "type": "feature",
      "scope": "Plugin"
    }
  ]
}
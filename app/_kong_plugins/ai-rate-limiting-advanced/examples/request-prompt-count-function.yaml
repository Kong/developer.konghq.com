description: |
  Protect your LLM services with rate limiting. 
  The AI Rate Limiting Advanced plugin will analyze query costs and token response 
  to provide an enterprise-grade rate limiting strategy.

  The following example uses request prompt rate limiting, which lets you you rate limit requests based on a custom token.

title: 'Request prompt function'

requirements:
  - "[AI Proxy plugin](/plugins/ai-proxy/) or [AI Proxy Advanced plugin](/plugins/ai-proxy-advanced/) configured with an LLM service"

weight: 900

config:
  llm_providers:
    - name: requestPrompt
      limit:
        - 100
        - 1000
      window_size:
        - 60
        - 3600
  request_prompt_count_function: "return #kong.request.get_raw_body()" 

tools:
  - deck
  - admin-api
  - konnect-api
  - kic
  - terraform

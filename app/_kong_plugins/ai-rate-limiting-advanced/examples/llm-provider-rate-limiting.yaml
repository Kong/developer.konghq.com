description: |
  Protect your LLM services with rate limiting. 
  The AI Rate Limiting Advanced plugin will analyze query costs and token response 
  to provide an enterprise-grade rate limiting strategy.

  The following example uses OpenAI, but you can apply the same strategies to any [supported LLM provider](/plugins/ai-rate-limiting-advanced/reference/#schema--config-llm-providers-name).

title: 'Enable LLM provider rate limiting'

requirements:
  - "[AI Proxy plugin](/plugins/ai-proxy/) or [AI Proxy Advanced plugin](/plugins/ai-proxy-advanced/) configured with an LLM service"

weight: 900

config:
  llm_providers:
    - name: openai
      limit:
        - 100
        - 1000
      window_size:
        - 60
        - 3600

tools:
  - deck
  - admin-api
  - konnect-api
  - kic
  - terraform

---
title: AI Rate Limiting Advanced

name: AI Rate Limiting Advanced
publisher: kong-inc

tier: ai_gateway_enterprise

products:
    - gateway

works_on:
    - on-prem
    - konnect

topologies:
  on_prem:
    - hybrid
    - db-less
    - traditional
  konnect_deployments:
    - hybrid
    - cloud-gateways
    - serverless

min_version:
  gateway: '3.7'

content_type: plugin
description: Provides rate limiting for the providers used by any AI plugins.
tags:
  - rate-limiting
  - traffic-control

icon: ai-rate-limiting-advanced.png

categories:
  - ai

search_aliases:
  - ai-rate-limiting-advanced

notes: |
  In DB-less, hybrid mode, and Konnect, the <code>cluster</code> config strategy
  is not supported. Use <code>redis</code> instead. In Serverless gateways only the
  <code>local</code> config strategy is supported.

related_resources:
  - text: Enforce AI rate limits in with a custom function
    url: /how-to/use-custom-function-for-ai-rate-limiting/
  - text: Use {{site.ai_gateway}} to govern GitHub MCP traffic
    url: /mcp/govern-mcp-traffic/

---

The AI Rate Limiting Advanced plugin provides rate limiting for the providers used by any AI plugins. The
AI Rate Limiting plugin extends the
[Rate Limiting Advanced](/plugins/rate-limiting-advanced/) plugin.

This plugin uses the token data returned by the LLM provider to calculate the costs of queries.
The same HTTP request can vary greatly in cost depending on the calculation of the
LLM providers.

A common pattern to protect your AI API is to analyze and assign costs to incoming queries, then rate limit the consumer's
cost for a given time window and provider.
You can also create a generic prompt rate limit using the [request prompt provider](#request-prompt-function).

Kong also provides multiple specialized rate limiting plugins, including rate limiting for service protection and on GraphQL queries.
See [Rate Limiting in {{site.base_gateway}}](/gateway/rate-limiting/) to choose the plugin that is most useful in your use case.

## Strategies

{% include_cached /plugins/rate-limiting/strategies.md name=page.name %}

{% include plugins/redis-cloud-auth.md %}

## Headers sent to the client

When this plugin is enabled, {{site.base_gateway}} sends some additional headers back to the client,
indicating the allowed limits, how many requests are available, and how long it will take
until the quota is restored. It also sends the limits in the time frame and the number
of remaining minutes for each provider.

For example:

```plaintext
X-AI-RateLimit-Reset: 47
X-AI-RateLimit-Retry-After: 47
X-AI-RateLimit-Limit-30-azure: 1000
X-AI-RateLimit-Remaining-30-azure: 950
```

You can optionally hide the limit and remaining headers with the [`config.hide_client_headers`](./reference/#schema--config-hide-client-headers) option.

If more than one limit is set, the plugin returns multiple time limit headers.
For example:

```plaintext
X-AI-RateLimit-Limit-30-azure: 1000
X-AI-RateLimit-Remaining-30-azure: 950
X-AI-RateLimit-Limit-40-cohere: 2000
X-AI-RateLimit-Remaining-40-cohere: 1150
```

If any of the limits are reached, the plugin returns an `HTTP/1.1 429` status
code to the client with the following JSON body:

```json
{ "message": "API rate limit exceeded for provider azure, cohere" }
```

For each provider, the plugin also indicates how long it will take until the quota is restored:

```plaintext
X-AI-RateLimit-Retry-After-30-azure: 1500
X-AI-RateLimit-Reset-30-azure: 1500
```

If using the request prompt provider, the plugin will send the query cost:

```plaintext
X-AI-RateLimit-Query-Cost: 100
```

The `Retry-After` headers will be present on `429` errors to indicate how long the service is
expected to be unavailable to the client. When using `window_type=sliding` and `RateLimit-Reset`, `Retry-After`
may increase due to the rate calculation for the sliding window.

{:.warning}
> The headers `RateLimit-Limit`, `RateLimit-Remaining`, and `RateLimit-Reset` are based on the Internet-Draft [RateLimit Header Fields for HTTP](https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers) and may change in the future to respect specification updates.

## Token count strategies

The plugin supports three strategies to calculate the number of tokens. Configure the strategy with [`tokens_count_strategy`](./reference/#schema--config-tokens-count-strategy).

{% table %}
columns:
  - title: Strategy
    key: strategy
  - title: Description
    key: description
rows:
  - strategy: "`total_tokens`"
    description: The total number of tokens in the request, including both prompt and completion tokens.
  - strategy: "`prompt_tokens`"
    description: The tokens provided as input to the LLM.
  - strategy: "`completion_tokens`"
    description: The tokens generated by the LLM in response to the prompt.
  - strategy: "`cost`"
    description: |
      {% new_in 3.8 %} The financial or computational cost incurred based on token usage. This strategy lets you limit API usage based on actual processing costs rather than raw token counts.
      <br><br>
      The plugin calculates cost as the sum of prompt tokens multiplied by input cost and completion tokens multiplied by output cost, divided by 1 million: `cost = (prompt_tokens × input_cost + completion_tokens × output_cost) / 1,000,000`.
      <br><br>
      You define `input_cost` and `output_cost` per 1 million tokens in whatever unit suits your use case, whether US dollars, cents, or internal billing credits. The rate limit threshold must use the same unit.
      <br><br>

      {:.warning}
      > This strategy requires `input_cost` and `output_cost` values in the [AI Proxy](/plugins/ai-proxy/) or [AI Proxy Advanced](/plugins/ai-proxy-advanced/) plugin configuration, under `model.options`.
{% endtable %}

### Request prompt function

You can decide to use a custom function to count the tokens for a requests.
To configure it, you must set the [`config.llm_providers.name`](./reference/#schema--config-llm-providers-name) to `requestPrompt` and specify the function in [`config.request_prompt_count_function`](./reference/#schema--config-request-prompt-count-function).

When using the request prompt provider, it will call the function to get the token count at the request level and implement a limit.

See the following [example configuration](/plugins/ai-rate-limiting-advanced/examples/request-prompt-count-function/) for more detail.

## Known limitations of AI Rate Limiting Advanced

The cost for the AI Proxy or AI Proxy Advanced is only reflected during the next request.

For example, if a request is made and the AI Proxy plugin returns a token cost of `100` for the `OpenAI` provider:
* The request is made to the OpenAI provider and the response is returned to the user
* If the rate limit is reached, the next request will be blocked

Additionally, [`config.disable_penalty`](./reference/#schema--config-disable-penalty) only works for the `requestPrompt` function.
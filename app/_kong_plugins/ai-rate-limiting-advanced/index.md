---
title: AI Rate Limiting Advanced

name: AI Rate Limiting Advanced
publisher: kong-inc

ai_gateway_enterprise: true

products:
    - gateway

works_on:
    - on-prem
    - konnect

topologies:
  on_prem:
    - hybrid
    - db-less
    - traditional
  konnect_deployments:
    - hybrid
    - cloud-gateways
    - serverless

min_version:
  gateway: '3.7'

content_type: plugin
description: Provides rate limiting for the providers used by any AI plugins.
tags:
  - rate-limiting
  - traffic-control

icon: ai-rate-limiting-advanced.png

categories:
  - ai

search_aliases:
  - ai-rate-limiting-advanced
---

The AI Rate Limiting Advanced plugin provides rate limiting for the providers used by any AI plugins. The
AI Rate Limiting plugin extends the
[Rate Limiting Advanced](/plugins/rate-limiting-advanced/) plugin.

This plugin uses the token data returned by the LLM provider to calculate the costs of queries.
The same HTTP request can vary greatly in cost depending on the calculation of the
LLM providers.

A common pattern to protect your AI API is to analyze and assign costs to incoming queries, then rate limit the consumer's
cost for a given time window and provider.
You can also create a generic prompt rate limit using the [request prompt provider](#request-prompt-function).

Kong also provides multiple specialized rate limiting plugins, including rate limiting for service protection and on GraphQL queries.
See [Rate Limiting in {{site.base_gateway}}](/gateway/rate-limiting/) to choose the plugin that is most useful in your use case.

## Strategies

{% include_cached /plugins/rate-limiting/strategies.md name=page.name %}

## Headers sent to the client

When this plugin is enabled, {{site.base_gateway}} sends some additional headers back to the client,
indicating the allowed limits, how many requests are available, and how long it will take
until the quota is restored. It also sends the limits in the time frame and the number
of remaining minutes for each provider.

For example:

```plaintext
X-AI-RateLimit-Reset: 47
X-AI-RateLimit-Retry-After: 47
X-AI-RateLimit-Limit-30-azure: 1000
X-AI-RateLimit-Remaining-30-azure: 950
```

You can optionally hide the limit and remaining headers with the [`config.hide_client_headers`](./reference/#schema--config-hide-client-headers) option.

If more than one limit is set, the plugin returns multiple time limit headers.
For example:

```plaintext
X-AI-RateLimit-Limit-30-azure: 1000
X-AI-RateLimit-Remaining-30-azure: 950
X-AI-RateLimit-Limit-40-cohere: 2000
X-AI-RateLimit-Remaining-40-cohere: 1150
```

If any of the limits are reached, the plugin returns an `HTTP/1.1 429` status
code to the client with the following JSON body:

```json
{ "message": "API rate limit exceeded for provider azure, cohere" }
```

For each provider, the plugin also indicates how long it will take until the quota is restored:

```plaintext
X-AI-RateLimit-Retry-After-30-azure: 1500
X-AI-RateLimit-Reset-30-azure: 1500
```

If using the request prompt provider, the plugin will send the query cost:

```plaintext
X-AI-RateLimit-Query-Cost: 100
```

The `Retry-After` headers will be present on `429` errors to indicate how long the service is
expected to be unavailable to the client. When using `window_type=sliding` and `RateLimit-Reset`, `Retry-After`
may increase due to the rate calculation for the sliding window.

{:.warning}
> The headers `RateLimit-Limit`, `RateLimit-Remaining`, and `RateLimit-Reset` are based on the Internet-Draft [RateLimit Header Fields for HTTP](https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers) and may change in the future to respect specification updates.

## Token count strategies

The plugin supports three strategies to calculate the number of tokens, configurable using the parameter [`tokens_count_strategy`](./reference/#schema--config-tokens-count-strategy):

{% table %}
columns:
  - title: Strategy
    key: strategy
  - title: Description
    key: description
rows:
  - strategy: "`total_tokens`"
    description: Represents the total number of tokens, including both the prompt and the generated completion, in the LLM's input sequence.
  - strategy: "`prompt_tokens`"
    description: Represents the tokens provided by the user as input to the LLM, typically defining the context or task.
  - strategy: "`completion_tokens`"
    description: Represents the tokens generated by the LLM in response to the prompt, representing the completed output or continuation of the task.
  - strategy: "`cost`"
    description: |
      {% new_in 3.8 %} Represents the financial or computational cost incurred based on the tokens used by the LLM during the request. Using this strategy can help you limit API usage based on the actual costs of processing the request, ensuring that expensive requests (in terms of token usage) are managed more carefully.
      <br><br>
      This cost is the sum of the number of prompt tokens multiplied by the cost per prompt token (input cost) and the number of completion tokens multiplied by the cost per completion token (output cost): `cost = prompt_tokens * input_cost + completion_tokens * output_cost`.
      <br><br>

      {:.warning}
      > To use this strategy, you must define the `config.input_cost` and `config.output_cost` in either the [AI Proxy](/plugins/ai-proxy/) or [AI Proxy Advanced plugin](/plugins/ai-proxy-advanced/).
{% endtable %}

### Request prompt function

You can decide to use a custom function to count the tokens for a requests.
To configure it, you must set the [`config.llm_providers.name`](./reference/#schema--config-llm-providers-name) to `requestPrompt` and specify the function in [`config.request_prompt_count_function`](./reference/#schema--config-request-prompt-count-function).

When using the request prompt provider, it will call the function to get the token count at the request level and implement a limit.

See the following [example configuration](/plugins/ai-rate-limiting-advanced/examples/request-prompt-count-function/) for more detail.

## Known limitations of AI Rate Limiting Advanced

The cost for the AI Proxy or AI Proxy Advanced is only reflected during the next request.

For example, if a request is made and the AI Proxy plugin returns a token cost of `100` for the `OpenAI` provider:
* The request is made to the OpenAI provider and the response is returned to the user
* If the rate limit is reached, the next request will be blocked

Additionally, [`config.disable_penalty`](./reference/#schema--config-disable-penalty) only works for the `requestPrompt` function.
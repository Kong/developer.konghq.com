{
  "3.13.0.0": [
    {
      "message": "Fixed an issue where cost savings attributes `ai_proxy_cache_cost_savings` were not properly calculated when cache hits.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where Cohere and Huggingface models did not work with the semantic cache because of the polluted request format.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the plugin did not report time to first token (TTFT) metrics when hit.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the plugin did not allow /responses api to be used with Azure.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.12.0.0": [
    {
      "message": "Fixed an issue where the DELETE `/ai-semantic-cache` endpoint returned 500 errors for successful deletions when using pgvector as the vector database.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the DELETE `/ai-semantic-cache` endpoint returned an incorrect 204 status for non-existent keys.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI inference requests to non-OpenAI providers in the stream mode were not supported.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the SSE terminator could have incorrect ending characters.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.12.0.1": [
    {
      "message": "Fixed an issue where Cohere and Huggingface models did not work with the semantic cache because of the polluted request format.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the plugin did not report time to first token (TTFT) metrics when hit.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.12.0.2": [
    {
      "message": "Fixed an issue where cost savings attributes `ai_proxy_cache_cost_savings` were not properly calculated when cache hits.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.0": [
    {
      "message": "Fixed an issue where some of ai metrics was missed in analytics",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed an issue where SSE terminator may not have correct ending characters.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Blocked plugins to execute retry logic. Also improve testing functions\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where some of ai metrics was missed in analytics",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "If any [AI Gateway plugin](/plugins/?category=ai) has been enabled in a self-managed Kong Gateway deployment for more than a week, upgrades from 3.10 versions to 3.11.0.0 will fail due to a license migration issue. This does not affect Konnect deployments.\n\nA fix will be provided in 3.11.0.1.\n\nSee [breaking changes in 3.11](/gateway/breaking-changes/#known-issues-in-3-11-0-0) for a temporary workaround.",
      "type": "known-issues",
      "scope": "Plugin"
    }
  ],
  "3.11.0.1": [
    {
      "message": "Fixed an issue where the llm license migration could fail if the license counter contained more than one week of data.",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed an issue where SSE terminator may not have correct ending characters.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.2": [
    {
      "message": "Fixed an issue where Gemini Vertex AI embeddings failed due to incorrect URL construction and response parsing.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.5": [
    {
      "message": "Fixed an issue where the DELETE /ai-semantic-cache returned 500 even deletion was successful, when using pgvector as vectordb.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the DELETE /ai-semantic-cache with non-existent key returned incorrect status 204.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where AI inference requests to non-OpenAI providers in the stream mode were not supported.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.11.0.7": [
    {
      "message": "Fixed an issue where Cohere and Huggingface models did not work with the semantic cache because of the polluted request format.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.0": [
    {
      "message": "Fixed an issue where AI upstream URL trailing would be empty.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Added support for boto3 SDKs for Bedrock provider, and for Google GenAI SDKs for Gemini provider.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Added the `huggingface`, `azure`, `vertex`, and `bedrock` providers to embeddings. They can be used by the ai-proxy-advanced, ai-semantic-cache, ai-semantic-prompt-guard, and ai-rag-injector plugins.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Allow authentication to Bedrock services with assume roles in AWS.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the Refresh header wasn't properly sent to the client.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed issue where the SSE body may have extra trailing.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.3": [
    {
      "message": "Fixed an issue where SSE terminator may not have correct ending characters.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.4": [
    {
      "message": "Fixed an issue where the Gemini provider could not use Anthropic 'rawPredict' endpoint models hosted in Vertex.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where Gemini Vertex AI embeddings failed due to incorrect URL construction and response parsing.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where SSE terminator may not have correct ending characters.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.1.1": [
    {
      "message": "Fixed issue of SSE body may have extra trailing in some cases.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.9.0.0": [
    {
      "message": "Fixed an bug that AI semantic cache can't use request provided models",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Made the\n`embeddings.model.name` config field a free text entry, enabling use of a\nself-hosted (or otherwise compatible) model.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed the exact matching to catch everything including embeddings.\n",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Added `ignore_tool` configuration option to discard tool role prompts from the input text.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Plugin can now be enabled on Consumer Groups.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the ai-semantic-cache plugin would abort in stream mode when another plugin enable the buffering proxy mode.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the ai-semantic-cache plugin put the wrong type value in the metrics when using the prometheus plugin.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the plugin failed when handling requests with multiple models.",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.8.1.0": [
    {
      "message": "Fixed an bug that AI semantic cache can't use request provided models",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the ai-semantic-cache plugin would abort in stream mode when another plugin enable the buffering proxy mode.",
      "type": "bugfix",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the ai-semantic-cache plugin put the wrong type value in the metrics when using the prometheus plugin.",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.8.0.0": [
    {
      "message": "allow AI plugin to read request from buffered file",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Introduced AI Semantic Caching plugin, enabling you \nto configure an embeddings-based caching system for Large Language Model responses.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fix the `ai-semantic-caching` plugin with a condition for calculating latencies when no embeddings, add deep copy for the request table and fix countback.",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ]
}
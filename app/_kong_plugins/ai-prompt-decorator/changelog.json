{
  "3.11.0.0": [
    {
      "message": "Fixed an issue where some of ai metrics was missed in analytics",
      "scope": "Plugin",
      "type": "bugfix"
    },
    {
      "message": "Fixed an issue where the `ai-prompt-decorator` plugin prompt field was too short.\n",
      "type": "feature",
      "scope": "Plugin"
    },
    {
      "message": "Fixed an issue where the plugin would miss the model, temperature, and other user-defined fields.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.10.0.0": [
    {
      "message": "Added support for boto3 SDKs for Bedrock provider, and for Google GenAI SDKs for Gemini provider.\n",
      "type": "feature",
      "scope": "Plugin"
    }
  ],
  "3.8.0.0": [
    {
      "message": "allow AI plugin to read request from buffered file",
      "type": "feature",
      "scope": "Plugin"
    }
  ],
  "3.7.0.0": [
    {
      "message": "Improve error handling in AI plugins.\n",
      "type": "bugfix",
      "scope": "Plugin"
    }
  ],
  "3.6.0.0": [
    {
      "message": "Introduced the new **AI Prompt Decorator** plugin that enables prepending and appending llm/v1/chat messages onto consumer LLM requests, for prompt tuning.",
      "type": "feature",
      "scope": "Plugin"
    }
  ]
}